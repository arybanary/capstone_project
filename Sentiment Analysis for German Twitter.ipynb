{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Sentiment Analysis of German Twitter\n",
    "\n",
    "This notebook is supposed to take a look at the data sued for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df = pd.read_csv(\"data/downloaded.tsv\", sep=\"\\t\", header=None, names=[\"id\", \"sentiment\", \"md5_hash\", \"alternative_ids\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9939"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "len(tweets_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dataset provided by the authors of the twitter sentiment corpus does only include the tweet ids. The actual tweets were loaded via the twitter api. Some of these are not available anymore, probably the authors deleted them or the accounts of the authors are deleted. We remove all tweets which are not available anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>md5_hash</th>\n",
       "      <th>alternative_ids</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367189542857482240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>c020aa23ff1f8ff985ce489b2b678674</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tainted Talents (Ateliertagebuch.) » Wir sind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>368327046574776321</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0096b66e311fffcca65c23d2a310083b</td>\n",
       "      <td>[]</td>\n",
       "      <td>Aber wenigstens kommt #Supernatural heute mal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>368309870673793024</td>\n",
       "      <td>neutral</td>\n",
       "      <td>575fd73efa41e07e2b0c360a721d19d7</td>\n",
       "      <td>[]</td>\n",
       "      <td>DARLEHEN - Angebot für Schufa-freie Darlehen: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>362896018389475328</td>\n",
       "      <td>neutral</td>\n",
       "      <td>8b824c765a7642a980b9e14c02830126</td>\n",
       "      <td>[]</td>\n",
       "      <td>ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>367912309303148545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a07f6bc77b0cfb06a75c7cb1a88d752b</td>\n",
       "      <td>[]</td>\n",
       "      <td>Na? Wo sind Frankens heimliche Talente? - Die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment                          md5_hash  \\\n",
       "3  367189542857482240   neutral  c020aa23ff1f8ff985ce489b2b678674   \n",
       "4  368327046574776321   neutral  0096b66e311fffcca65c23d2a310083b   \n",
       "6  368309870673793024   neutral  575fd73efa41e07e2b0c360a721d19d7   \n",
       "7  362896018389475328   neutral  8b824c765a7642a980b9e14c02830126   \n",
       "8  367912309303148545   neutral  a07f6bc77b0cfb06a75c7cb1a88d752b   \n",
       "\n",
       "  alternative_ids                                               text  \n",
       "3              []  Tainted Talents (Ateliertagebuch.) » Wir sind ...  \n",
       "4              []  Aber wenigstens kommt #Supernatural heute mal ...  \n",
       "6              []  DARLEHEN - Angebot für Schufa-freie Darlehen: ...  \n",
       "7              []  ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...  \n",
       "8              []  Na? Wo sind Frankens heimliche Talente? - Die ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = tweets_df[tweets_df['text'] != 'Not Available']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6539"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets total: 6539\n",
      "Positive tweets: 1480\n",
      "Neutral tweets: 4078\n",
      "Negative tweets: 981\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = cleaned_df[cleaned_df['sentiment']=='positive']\n",
    "neutral_tweets = cleaned_df[cleaned_df['sentiment']=='neutral']\n",
    "negative_tweets = cleaned_df[cleaned_df['sentiment']=='negative']\n",
    "\n",
    "print(f'Tweets total: {len(cleaned_df.index)}')\n",
    "print(f'Positive tweets: {len(positive_tweets.index)}')\n",
    "print(f'Neutral tweets: {len(neutral_tweets.index)}')\n",
    "print(f'Negative tweets: {len(negative_tweets.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of sentiments\n",
    "ax = cleaned_df['sentiment'].value_counts().plot.bar()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('./figure.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to categorize the senitments, we replace the sentiment categories which are actual words ('positive', 'neutral', negative') with numbers. We use \n",
    "- 2 for positive tweets\n",
    "- 1 for neutral tweets\n",
    "- 0 for negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "cleaned_df['sentiment'] = cleaned_df['sentiment'].apply(\n",
    "      lambda x: 2 if x == 'positive' else (0 if x == 'negative' else 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model in a later step, we need to divide the data into tranining and testing data. We define a method which does that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitting(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.1, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = splitting(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first tweets of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     Tainted Talents (Ateliertagebuch.) » Wir sind ...\n",
       "4     Aber wenigstens kommt #Supernatural heute mal ...\n",
       "6     DARLEHEN - Angebot für Schufa-freie Darlehen: ...\n",
       "7     ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...\n",
       "8     Na? Wo sind Frankens heimliche Talente? - Die ...\n",
       "9                        ... Glück breitet sich aus ...\n",
       "10                           @Moramee unachtsam *seufz*\n",
       "11    #jobs #Sales #Medien #TV #Tele M1 sucht #Persö...\n",
       "13    Happy Halloween liebe Studenten, zeigt eure gr...\n",
       "14    Die App „iDelete“ hat mir 3,4 MB belegten Spei...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy Halloween liebe Studenten, zeigt eure gruseligen Seiten!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will reduce the noise within the tweets. We will also bring the tweets in a form which is of use for machine learning algorithms. To get as much information as possible form the data set, we will make use of many Natural language Processing specific processing steps.\n",
    "\n",
    "One step is stemming, the process of reducing words to their word stem. This is a very language specific task. There are several stemmers specifically for the German language these one perform better than general ones. Cistem has shown to be most effective for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem.cistem import Cistem\n",
    "\n",
    "stemmer = Cistem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is crucial for working with the tweets. We will:\n",
    "- Reduce noise (remove 'rt' for retweets from the tweets)\n",
    "- Unify some text elements (replace mentionings and links with a representative for that specific class)\n",
    "- Tokenize sentences using a twitter sepcific tokenizer\n",
    "- Stem words using the previously defined stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "def review_to_words(text):\n",
    "    text = text.lower() # Convert to lower case\n",
    "    text = re.sub(r\"http\\S+\", \"XXXURLTOKENXXX\", text) # replace links with token\n",
    "    text = re.sub(r\"@\\S+\", \"XXXUSERNAMETOKENXXX\", text) # replace mentions with token\n",
    "    text = re.sub(r'[.]{2,}', ' ... ', text) # unify consecutive periods if there are at least 2\n",
    "    text = re.sub(\"(?:(?<=^)|(?<=\\s))(\\d+[.,]*)+(?=$|\\s)\", \"\", text) # remove all numbers not being part of alphanumeric word\n",
    "    text = re.sub(r\"#\", \"\", text) # remove '#' in front of hashtags\n",
    "    text = re.sub(r\"rt \", \"\", text) # remove 'RT'\n",
    "    words = tweet_tokenizer.tokenize(text)\n",
    "    words = [w for w in words if w not in stopwords.words(\"german\")] # Remove stopwords\n",
    "    words = [stemmer.stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our preprocessing step works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...',\n",
       " '...',\n",
       " 'wunderscho',\n",
       " 'tag',\n",
       " '!',\n",
       " 'toll',\n",
       " '!',\n",
       " '!',\n",
       " '*',\n",
       " 'seufz',\n",
       " '*',\n",
       " '<3',\n",
       " '<3',\n",
       " ':-)',\n",
       " 'xxxusernametokenxxx']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Das... ist ein.....  wunderschöner Tag! Toll!! 77.7 *seufz* <3<3 :-) @mama'\n",
    "review_to_words(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After havind defined a routine to preprocess and split our tweets into lists of strings, we need to apply it to our datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_train, data_test):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # Preprocess training and test data to obtain words for each review\n",
    "    words_train = [review_to_words(review) for review in data_train]\n",
    "    words_test = [review_to_words(review) for review in data_test]\n",
    "    \n",
    "    return words_train, words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data looks after this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tainted',\n",
       "  'tal',\n",
       "  '(',\n",
       "  'ateliertagebuch',\n",
       "  '.',\n",
       "  ')',\n",
       "  '»',\n",
       "  'allei',\n",
       "  'xxxurltokenxxx'],\n",
       " ['wenig', 'komm', 'supernatural', 'heu', 'mal', 'uhr', '-', 'schwach', 'tro'],\n",
       " ['darleh',\n",
       "  '-',\n",
       "  'angebo',\n",
       "  'schufa-frei',\n",
       "  'darleh',\n",
       "  ':',\n",
       "  'gunstig',\n",
       "  'anbie',\n",
       "  'onli',\n",
       "  'darleh',\n",
       "  'deutschla',\n",
       "  'xxxurltokenxxx'],\n",
       " ['anruf',\n",
       "  'erwunsch',\n",
       "  ':',\n",
       "  'hardcor',\n",
       "  'teeny',\n",
       "  'vicky',\n",
       "  'carrera',\n",
       "  ':',\n",
       "  'hallo',\n",
       "  'suss',\n",
       "  ',',\n",
       "  'jahrig',\n",
       "  'vicky',\n",
       "  'deutschla',\n",
       "  '...',\n",
       "  'xxxurltokenxxx'],\n",
       " ['na',\n",
       "  '?',\n",
       "  'frank',\n",
       "  'heimlich',\n",
       "  'tal',\n",
       "  '?',\n",
       "  '-',\n",
       "  'ers',\n",
       "  'bewerbung',\n",
       "  'talentwettbewerb',\n",
       "  '\"',\n",
       "  'wer',\n",
       "  'ko',\n",
       "  ',',\n",
       "  'darf',\n",
       "  '!',\n",
       "  '\"',\n",
       "  '...',\n",
       "  'xxxurltokenxxx'],\n",
       " ['...', 'gluck', 'brei', '...'],\n",
       " ['xxxusernametokenxxx', 'unachtsam', '*', 'seufz', '*'],\n",
       " ['job',\n",
       "  'sal',\n",
       "  'medie',\n",
       "  'tv',\n",
       "  'tel',\n",
       "  'm1',\n",
       "  'such',\n",
       "  'personlichkei',\n",
       "  'werb',\n",
       "  'verkauf',\n",
       "  'aussendie',\n",
       "  '(',\n",
       "  'm',\n",
       "  '/',\n",
       "  'w',\n",
       "  ')',\n",
       "  'xxxurltokenxxx'],\n",
       " ['happy', 'hallowee', 'lieb', 'stud', ',', 'zeig', 'gruselig', 'seit', '!'],\n",
       " ['app',\n",
       "  '„',\n",
       "  'idel',\n",
       "  '“',\n",
       "  'mb',\n",
       "  'beleg',\n",
       "  'speich',\n",
       "  'freigemach',\n",
       "  '.',\n",
       "  'losch',\n",
       "  'app',\n",
       "  'bring',\n",
       "  'mb',\n",
       "  'frei',\n",
       "  'speich',\n",
       "  '.',\n",
       "  'tschuss',\n",
       "  '!']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the most frequent text elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xxxusernametokenxxx', 3579), ('.', 2772), ('xxxurltokenxxx', 2757), (',', 2491), (':', 2155), ('!', 1541), ('...', 1203), ('?', 820), ('\"', 711), ('-', 637), ('(', 437), (')', 408), ('*', 277), ('gut', 273), ('/', 245), ('…', 242), ('neu', 241), ('schon', 219), ('mehr', 203), ('mach', 169), (\"'\", 156), ('+', 150), ('dank', 150), ('mal', 147), (':)', 147), ('geh', 140), ('heu', 131), ('ie', 129), ('einfach', 119), ('gross', 115), ('bitt', 115), ('ja', 112), ('lieb', 110), ('^', 109), ('|', 107), ('klei', 101), (';)', 97), ('morg', 96), ('war', 94), ('komm', 92), ('“', 91), ('ne', 91), ('such', 87), ('ganz', 87), (':-)', 82), ('hal', 82), ('immer', 77), ('wohl', 77), ('jahr', 74), ('deutsch', 73)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_words = get_all_words(X_train)\n",
    "freq_dist = FreqDist(all_words)\n",
    "print(freq_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_features(words_train, words_test, vocabulary_size=2000):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "\n",
    "    # Fit a vectorizer to training documents and use it to transform them\n",
    "    # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "    #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "    vectorizer = TfidfVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "    features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "    # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "    features_test = vectorizer.transform(words_test).toarray()\n",
    "    \n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "X_train, X_test, vocabulary = extract_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tal': 1626,\n",
       " '(': 3,\n",
       " '.': 11,\n",
       " ')': 5,\n",
       " '»': 1973,\n",
       " 'allei': 112,\n",
       " 'xxxurltokenxxx': 1923,\n",
       " 'wenig': 1858,\n",
       " 'komm': 944,\n",
       " 'heu': 770,\n",
       " 'mal': 1058,\n",
       " 'uhr': 1712,\n",
       " '-': 10,\n",
       " 'schwach': 1475,\n",
       " 'darleh': 366,\n",
       " 'angebo': 128,\n",
       " ':': 48,\n",
       " 'gunstig': 708,\n",
       " 'anbie': 121,\n",
       " 'onli': 1251,\n",
       " 'deutschla': 395,\n",
       " 'anruf': 133,\n",
       " 'erwunsch': 529,\n",
       " 'hallo': 725,\n",
       " 'suss': 1617,\n",
       " ',': 9,\n",
       " 'jahrig': 870,\n",
       " '...': 14,\n",
       " 'na': 1150,\n",
       " '?': 71,\n",
       " 'frank': 610,\n",
       " 'heimlich': 752,\n",
       " 'ers': 525,\n",
       " '\"': 1,\n",
       " 'wer': 1860,\n",
       " 'darf': 364,\n",
       " '!': 0,\n",
       " 'gluck': 676,\n",
       " 'brei': 291,\n",
       " 'xxxusernametokenxxx': 1924,\n",
       " '*': 7,\n",
       " 'job': 880,\n",
       " 'sal': 1416,\n",
       " 'medie': 1077,\n",
       " 'tv': 1699,\n",
       " 'tel': 1637,\n",
       " 'such': 1612,\n",
       " 'verkauf': 1763,\n",
       " 'm': 1044,\n",
       " '/': 15,\n",
       " 'w': 1812,\n",
       " 'happy': 734,\n",
       " 'hallowee': 726,\n",
       " 'lieb': 1019,\n",
       " 'stud': 1604,\n",
       " 'zeig': 1939,\n",
       " 'seit': 1489,\n",
       " 'app': 143,\n",
       " '„': 1980,\n",
       " '“': 1978,\n",
       " 'mb': 1073,\n",
       " 'speich': 1549,\n",
       " 'losch': 1035,\n",
       " 'bring': 293,\n",
       " 'frei': 616,\n",
       " 'moi': 1121,\n",
       " 'check': 330,\n",
       " 'out': 1265,\n",
       " 'the': 1653,\n",
       " 'new': 1191,\n",
       " 'pag': 1271,\n",
       " 'at': 160,\n",
       " 'tim': 1659,\n",
       " 'fir': 589,\n",
       " 'world': 1909,\n",
       " 'schau': 1438,\n",
       " 'neu': 1187,\n",
       " 'abend': 83,\n",
       " 'ler': 1010,\n",
       " 'erhoh': 512,\n",
       " 'teur': 1646,\n",
       " 'schneid': 1459,\n",
       " 'a': 81,\n",
       " 's': 1408,\n",
       " 'c': 314,\n",
       " 'mull': 1136,\n",
       " 'inter': 844,\n",
       " 'nerv': 1182,\n",
       " 'ie': 820,\n",
       " 'v': 1744,\n",
       " 'viel': 1800,\n",
       " 'detail': 391,\n",
       " 'vorh': 1808,\n",
       " 'bad': 189,\n",
       " 'endlich': 486,\n",
       " 'wundervoll': 1914,\n",
       " 'hm': 781,\n",
       " 'vielleich': 1801,\n",
       " 'schal': 1433,\n",
       " 'einfach': 467,\n",
       " ':)': 53,\n",
       " 'googl': 686,\n",
       " '+': 8,\n",
       " 'kurz': 974,\n",
       " 'vorgestell': 1807,\n",
       " 'bereich': 221,\n",
       " 'exper': 544,\n",
       " 'manag': 1059,\n",
       " '…': 1982,\n",
       " 'berich': 223,\n",
       " 'spd': 1548,\n",
       " 'lass': 992,\n",
       " 'gross': 698,\n",
       " 'koalitio': 937,\n",
       " 'platz': 1298,\n",
       " 'schnell': 1460,\n",
       " 'of': 1235,\n",
       " 'fuck': 632,\n",
       " 'dow': 424,\n",
       " 'you': 1929,\n",
       " 'lik': 1024,\n",
       " 'sonn': 1536,\n",
       " 'spa': 1544,\n",
       " 'ess': 533,\n",
       " 'and': 122,\n",
       " 'lis': 1027,\n",
       " 'on': 1249,\n",
       " '3': 34,\n",
       " 'dach': 354,\n",
       " 'has': 737,\n",
       " 'ach': 91,\n",
       " 'ja': 866,\n",
       " ':d': 60,\n",
       " 'd': 351,\n",
       " 'bekomm': 213,\n",
       " 'ne': 1171,\n",
       " 'hoh': 792,\n",
       " 'max': 1071,\n",
       " '«': 1970,\n",
       " 'frag': 609,\n",
       " 'ding': 409,\n",
       " 'schatt': 1436,\n",
       " 'hmmm': 783,\n",
       " 'fehl': 572,\n",
       " 'glaub': 673,\n",
       " 'falsch': 557,\n",
       " ':-)': 55,\n",
       " 'sich': 1509,\n",
       " 'druck': 439,\n",
       " '▸': 1984,\n",
       " 'arbei': 145,\n",
       " 'sozial': 1543,\n",
       " '•': 1981,\n",
       " 'immer': 831,\n",
       " 'mehr': 1080,\n",
       " 'mensch': 1089,\n",
       " 'ab': 82,\n",
       " 'wa': 1813,\n",
       " ':/': 58,\n",
       " 'geh': 658,\n",
       " 'fb': 568,\n",
       " 'einsatz': 471,\n",
       " 'b': 186,\n",
       " 'schenk': 1442,\n",
       " '—': 1975,\n",
       " 'o': 1229,\n",
       " 'voll': 1804,\n",
       " 'samstag': 1420,\n",
       " 'wel': 1852,\n",
       " 'via': 1797,\n",
       " 'offentlich': 1238,\n",
       " 'hand': 730,\n",
       " 'pos': 1315,\n",
       " 'grati': 691,\n",
       " '4': 40,\n",
       " '✔': 1991,\n",
       " 'winn': 1884,\n",
       " 'winnspiel': 1885,\n",
       " 'frisch': 626,\n",
       " 'gar': 654,\n",
       " '❤': 1992,\n",
       " 'ern': 518,\n",
       " 'herb': 763,\n",
       " 'herr': 764,\n",
       " 'kai': 894,\n",
       " 'mach': 1047,\n",
       " 'karrier': 903,\n",
       " 'leer': 1002,\n",
       " 'ick': 817,\n",
       " 'hoff': 788,\n",
       " 'fall': 556,\n",
       " 'verruck': 1779,\n",
       " 'sieh': 1515,\n",
       " 'zeit': 1940,\n",
       " 'berli': 224,\n",
       " 'seitensprung': 1490,\n",
       " 'wunsch': 1915,\n",
       " 'wahr': 1820,\n",
       " 'probier': 1327,\n",
       " 'bitt': 260,\n",
       " 'nix': 1207,\n",
       " 'hab': 716,\n",
       " 'erreich': 523,\n",
       " 'beginn': 207,\n",
       " 'kenn': 909,\n",
       " 'accou': 90,\n",
       " 'much': 1133,\n",
       " 'okay': 1245,\n",
       " 'super': 1616,\n",
       " 'end': 485,\n",
       " '😉': 1996,\n",
       " 'strom': 1601,\n",
       " 'wurd': 1916,\n",
       " 'grund': 702,\n",
       " 'schloss': 1453,\n",
       " 'snowd': 1529,\n",
       " 'dien': 405,\n",
       " 'benutz': 216,\n",
       " '\\\\': 74,\n",
       " 'vergess': 1757,\n",
       " 'konig': 950,\n",
       " 'ganz': 653,\n",
       " 'tag': 1624,\n",
       " 'fahr': 551,\n",
       " 'sooo': 1539,\n",
       " 'gut': 709,\n",
       " 'warum': 1829,\n",
       " 'romantisch': 1388,\n",
       " 'empfehlung': 483,\n",
       " 'bald': 192,\n",
       " 'moglich': 1119,\n",
       " 'upda': 1736,\n",
       " 'if': 821,\n",
       " 'today': 1668,\n",
       " 'your': 1931,\n",
       " 'las': 990,\n",
       " 'day': 373,\n",
       " 'interessa': 846,\n",
       " 'dabei': 353,\n",
       " 'konkr': 951,\n",
       " 'perso': 1286,\n",
       " 'mein': 1081,\n",
       " 'offenbar': 1237,\n",
       " 'schein': 1440,\n",
       " 'davo': 371,\n",
       " 'mollath': 1122,\n",
       " 'helf': 759,\n",
       " 'to': 1666,\n",
       " 'sleep': 1523,\n",
       " 'nach': 1151,\n",
       " 'heck': 746,\n",
       " 'well': 1854,\n",
       " 'ech': 453,\n",
       " 'ausprobier': 177,\n",
       " 'wett': 1865,\n",
       " 'serie': 1495,\n",
       " 'brauch': 288,\n",
       " 'osterreich': 1263,\n",
       " 'auftrag': 170,\n",
       " 'politik': 1309,\n",
       " 'nrw': 1220,\n",
       " '13': 23,\n",
       " 'frau': 613,\n",
       " 'kaputt': 901,\n",
       " 'unbeding': 1716,\n",
       " 'richtig': 1376,\n",
       " 'facebook': 549,\n",
       " 'unglucklich': 1721,\n",
       " 'konn': 952,\n",
       " 'bestatig': 232,\n",
       " 'i': 814,\n",
       " 'com': 344,\n",
       " 'leipzig': 1007,\n",
       " 'nein': 1177,\n",
       " 'politisch': 1310,\n",
       " 'bleib': 265,\n",
       " 'meld': 1086,\n",
       " 'morg': 1127,\n",
       " 'fruh': 629,\n",
       " 'los': 1034,\n",
       " 'spiel': 1553,\n",
       " 'nochmal': 1209,\n",
       " 'ok': 1244,\n",
       " 'dank': 360,\n",
       " 'steh': 1576,\n",
       " 'drauss': 430,\n",
       " 'schon': 1462,\n",
       " 'mag': 1051,\n",
       " 'ster': 1583,\n",
       " 'notig': 1216,\n",
       " 'genug': 663,\n",
       " 'schrott': 1470,\n",
       " 'unmoglich': 1724,\n",
       " ';)': 64,\n",
       " 'buch': 299,\n",
       " 'indisch': 835,\n",
       " 'mi': 1096,\n",
       " '.  ...': 12,\n",
       " 'lang': 985,\n",
       " 'intensiv': 843,\n",
       " 'traum': 1682,\n",
       " 'schlaf': 1445,\n",
       " 'ho': 784,\n",
       " 'seh': 1486,\n",
       " 'erfolgreich': 507,\n",
       " 'air': 103,\n",
       " 'jema': 876,\n",
       " 'tipp': 1661,\n",
       " 'neuer': 1188,\n",
       " 'lahm': 983,\n",
       " 'mertesack': 1095,\n",
       " 'boateng': 275,\n",
       " 'khedira': 913,\n",
       " 'ozil': 1267,\n",
       " 'klo': 932,\n",
       " 'mog': 1118,\n",
       " 'verstarkung': 1787,\n",
       " 'justi': 890,\n",
       " '–': 1974,\n",
       " 'jahr': 868,\n",
       " 'wohl': 1901,\n",
       " 'sportlich': 1558,\n",
       " 'gonn': 684,\n",
       " 'termi': 1642,\n",
       " 'pau': 1278,\n",
       " 'gabriel': 648,\n",
       " 'kind': 917,\n",
       " 'soll': 1534,\n",
       " 'hilf': 776,\n",
       " 'erledig': 517,\n",
       " 'regierung': 1359,\n",
       " 'sel': 1491,\n",
       " 'sundhei': 1615,\n",
       " 'le': 996,\n",
       " 'aktuell': 108,\n",
       " 'klei': 929,\n",
       " 'pho': 1290,\n",
       " 'call': 316,\n",
       " 'now': 1218,\n",
       " 'or': 1256,\n",
       " 'scheiss': 1441,\n",
       " 'drauf': 429,\n",
       " 'anfang': 125,\n",
       " 'nutz': 1228,\n",
       " '”': 1979,\n",
       " 'is': 857,\n",
       " 'lieber': 1020,\n",
       " 'teuer': 1645,\n",
       " 'aufgeb': 168,\n",
       " ';-)': 65,\n",
       " 'ist': 859,\n",
       " 'n': 1148,\n",
       " 'versuch': 1790,\n",
       " 'positiv': 1316,\n",
       " 'denk': 381,\n",
       " '8:': 47,\n",
       " 'arm': 151,\n",
       " 'baby': 187,\n",
       " 'klau': 928,\n",
       " 'hamburg': 727,\n",
       " 'pla': 1296,\n",
       " 'autor': 184,\n",
       " 'sitz': 1521,\n",
       " 'schreib': 1465,\n",
       " 'unsich': 1727,\n",
       " 'lach': 979,\n",
       " 'ha': 714,\n",
       " 'ebe': 450,\n",
       " 'innovatio': 841,\n",
       " 'blau': 264,\n",
       " 'danach': 359,\n",
       " 'welcom': 1853,\n",
       " 'back': 188,\n",
       " 'roll': 1386,\n",
       " 'novemb': 1217,\n",
       " 'sonntag': 1537,\n",
       " 'mona': 1124,\n",
       " 'eigentlich': 463,\n",
       " 'merk': 1092,\n",
       " 'off': 1236,\n",
       " 'desig': 389,\n",
       " 'schaf': 1431,\n",
       " 'my': 1147,\n",
       " 'witzig': 1897,\n",
       " 'link': 1026,\n",
       " 'folg': 597,\n",
       " 'hat': 740,\n",
       " '[': 73,\n",
       " ']': 75,\n",
       " 'be': 200,\n",
       " 'band': 194,\n",
       " 'musik': 1140,\n",
       " 'freu': 621,\n",
       " 'twee': 1702,\n",
       " '=': 69,\n",
       " 'idio': 819,\n",
       " 'kauf': 906,\n",
       " \":'d\": 51,\n",
       " 'hahah': 719,\n",
       " 'deutlich': 393,\n",
       " 'bor': 280,\n",
       " 'phil': 1289,\n",
       " 'freundi': 623,\n",
       " 'nich': 1194,\n",
       " '2-0': 28,\n",
       " 'stark': 1572,\n",
       " 'hoher': 793,\n",
       " 'weg': 1841,\n",
       " 'beid': 208,\n",
       " 'spor': 1557,\n",
       " 'math': 1069,\n",
       " 'hey': 771,\n",
       " 'kostenlo': 957,\n",
       " 'sex': 1498,\n",
       " 'cha': 325,\n",
       " 'dieb': 404,\n",
       " 'aktiv': 106,\n",
       " 'englisch': 491,\n",
       " 'ball': 193,\n",
       " 'heiss': 756,\n",
       " 'wuss': 1917,\n",
       " 'dienstag': 406,\n",
       " 'wiener': 1873,\n",
       " 'f': 547,\n",
       " 'wien': 1872,\n",
       " 'eve': 539,\n",
       " 'toll': 1670,\n",
       " '<3': 68,\n",
       " 'schlimm': 1452,\n",
       " 'langsam': 987,\n",
       " 'cdu': 323,\n",
       " 'girl': 671,\n",
       " 'geil': 659,\n",
       " 'nett': 1185,\n",
       " 'troff': 1690,\n",
       " 'foto': 606,\n",
       " 'videoday': 1799,\n",
       " 'alter': 115,\n",
       " 'krank': 959,\n",
       " '|': 1967,\n",
       " 'fas': 564,\n",
       " 'fit': 591,\n",
       " 'bar': 197,\n",
       " 'review': 1373,\n",
       " 'kevi': 911,\n",
       " 'ost': 1262,\n",
       " 'ger': 665,\n",
       " '1:0': 25,\n",
       " 'hertha': 766,\n",
       " 'bsc': 297,\n",
       " 'eintrach': 473,\n",
       " 'frankfur': 612,\n",
       " 'don': 420,\n",
       " 'cool': 347,\n",
       " 'nimm': 1205,\n",
       " 'big': 251,\n",
       " 'keith': 908,\n",
       " '25': 32,\n",
       " 'vertrag': 1792,\n",
       " 'boah': 274,\n",
       " 'nd': 1169,\n",
       " 'sag': 1413,\n",
       " 'dumm': 442,\n",
       " 'auf': 166,\n",
       " 'hass': 738,\n",
       " 'forderung': 603,\n",
       " 'deutsch': 394,\n",
       " '2013': 29,\n",
       " 'rad': 1339,\n",
       " 'sech': 1483,\n",
       " 'angenehm': 130,\n",
       " 'wahlkampf': 1818,\n",
       " 'spass': 1547,\n",
       " 'fdp': 571,\n",
       " 'weltbild': 1855,\n",
       " 'zerstor': 1942,\n",
       " 'arsch': 152,\n",
       " 'lustig': 1043,\n",
       " 'plaka': 1297,\n",
       " 'bundesliga': 303,\n",
       " 'stopp': 1590,\n",
       " 'vorbei': 1806,\n",
       " 'sinn': 1520,\n",
       " 'hal': 722,\n",
       " 'fress': 620,\n",
       " 'leg': 1003,\n",
       " 'tut': 1697,\n",
       " 'video': 1798,\n",
       " \":'(\": 49,\n",
       " 'durf': 444,\n",
       " 'real': 1350,\n",
       " 'madrid': 1050,\n",
       " 'verlass': 1765,\n",
       " 'birthday': 256,\n",
       " 'tier': 1657,\n",
       " 'drog': 437,\n",
       " 'bestell': 234,\n",
       " 'gleich': 674,\n",
       " 'aufschrei': 169,\n",
       " 'tot': 1674,\n",
       " 'wunderscho': 1913,\n",
       " 'kredi': 961,\n",
       " 'aktio': 105,\n",
       " 'war': 1827,\n",
       " 'darauf': 363,\n",
       " 'chanc': 328,\n",
       " 'schlech': 1449,\n",
       " 'ahnung': 102,\n",
       " 'mutti': 1146,\n",
       " 'wird': 1886,\n",
       " 'rich': 1375,\n",
       " 'deshalb': 388,\n",
       " 'gru': 701,\n",
       " 'naturlich': 1166,\n",
       " 'afd': 95,\n",
       " 'fussball': 645,\n",
       " 'stuttgar': 1610,\n",
       " 'kick': 914,\n",
       " 'calvi': 317,\n",
       " 'x': 1919,\n",
       " 'extr': 545,\n",
       " 'slim': 1524,\n",
       " 'schad': 1430,\n",
       " \"'\": 2,\n",
       " 'bayer': 199,\n",
       " 'leverku': 1016,\n",
       " 'ubrig': 1710,\n",
       " 'nintendo': 1206,\n",
       " '2ds': 33,\n",
       " 'zwei': 1964,\n",
       " 'johnny': 882,\n",
       " 'bedeu': 205,\n",
       " 'fal': 555,\n",
       " 'leistung': 1009,\n",
       " ':(': 52,\n",
       " 'sprech': 1560,\n",
       " 'dick': 402,\n",
       " 'vater': 1746,\n",
       " 'quo': 1337,\n",
       " 'wieso': 1875,\n",
       " 'pira': 1294,\n",
       " 'soo': 1538,\n",
       " 'bess': 230,\n",
       " \"i'm\": 815,\n",
       " 'lauf': 995,\n",
       " 'gott': 687,\n",
       " 'stimm': 1586,\n",
       " 'sund': 1614,\n",
       " 'ruck': 1398,\n",
       " 'je': 874,\n",
       " 'eur': 536,\n",
       " 'amazo': 119,\n",
       " 'luf': 1040,\n",
       " 'ohr': 1243,\n",
       " 'zieh': 1944,\n",
       " 'hopp': 797,\n",
       " 'mann': 1062,\n",
       " 'reich': 1361,\n",
       " 'bock': 277,\n",
       " 'tiger': 1658,\n",
       " 'sowa': 1542,\n",
       " 'chef': 331,\n",
       " 'muss': 1142,\n",
       " 'beschw': 227,\n",
       " 'u': 1705,\n",
       " 'fang': 561,\n",
       " 'inhal': 839,\n",
       " 'urteil': 1739,\n",
       " 'erfolg': 506,\n",
       " 'har': 735,\n",
       " 'tft': 1649,\n",
       " 'zoll': 1949,\n",
       " 'led': 1000,\n",
       " 'weiss': 1848,\n",
       " 'leider': 1006,\n",
       " 'schick': 1444,\n",
       " 'euro': 537,\n",
       " 'laptop': 989,\n",
       " 'haha': 718,\n",
       " 'schweiz': 1478,\n",
       " 'bea': 201,\n",
       " 'journali': 883,\n",
       " 'rech': 1352,\n",
       " 'energiew': 488,\n",
       " 'steck': 1575,\n",
       " 'tru': 1693,\n",
       " 'lov': 1037,\n",
       " 'fun': 639,\n",
       " 'kiss': 921,\n",
       " 'win': 1881,\n",
       " 'kal': 895,\n",
       " 'pink': 1293,\n",
       " 'bla': 262,\n",
       " 'lachel': 980,\n",
       " 'besserung': 231,\n",
       " 'get': 666,\n",
       " 'gladbach': 672,\n",
       " 'ile': 827,\n",
       " 'fuhl': 634,\n",
       " 'krass': 960,\n",
       " 'gluckwunsch': 678,\n",
       " \":')\": 50,\n",
       " 'porsch': 1314,\n",
       " 'alt': 114,\n",
       " 'spann': 1545,\n",
       " 'numm': 1226,\n",
       " 'kri': 962,\n",
       " 'kim': 916,\n",
       " 'verlobung': 1771,\n",
       " 'photo': 1291,\n",
       " 'westfalisch': 1864,\n",
       " 'bielefeld': 248,\n",
       " '):': 6,\n",
       " 'schwer': 1479,\n",
       " 'aussich': 181,\n",
       " 'wenn': 1859,\n",
       " 'komisch': 943,\n",
       " 'farb': 563,\n",
       " 'entspann': 499,\n",
       " 'aug': 171,\n",
       " 'studie': 1605,\n",
       " 'kita': 922,\n",
       " 'einstei': 472,\n",
       " 'weiterhi': 1851,\n",
       " 'hol': 794,\n",
       " 'hor': 798,\n",
       " 'hamm': 728,\n",
       " 'bot': 283,\n",
       " 'stell': 1580,\n",
       " 'zuruck': 1960,\n",
       " 'uber': 1706,\n",
       " 'handy': 731,\n",
       " 'rechnung': 1353,\n",
       " 'verdamm': 1753,\n",
       " 'aha': 100,\n",
       " 'les': 1011,\n",
       " 'schrecklich': 1464,\n",
       " 'ovp': 1266,\n",
       " 'ever': 540,\n",
       " 'gon': 683,\n",
       " 'unterschied': 1730,\n",
       " 'good': 685,\n",
       " 'morning': 1128,\n",
       " 'nic': 1193,\n",
       " '@': 72,\n",
       " 'nauso': 1167,\n",
       " 'moder': 1117,\n",
       " '^': 77,\n",
       " 'frech': 614,\n",
       " 'hubsch': 806,\n",
       " 'montag': 1125,\n",
       " 'per': 1284,\n",
       " 'ausserd': 180,\n",
       " 'all': 111,\n",
       " 'story': 1592,\n",
       " 'exklusiv': 543,\n",
       " 'klassisch': 927,\n",
       " 'hau': 742,\n",
       " 'wiss': 1893,\n",
       " 'mon': 1123,\n",
       " 'streich': 1598,\n",
       " 'mis': 1107,\n",
       " 'drei': 433,\n",
       " 'dankescho': 361,\n",
       " 'ff': 584,\n",
       " 'gruss': 703,\n",
       " 'full': 638,\n",
       " 'furch': 644,\n",
       " 'grad': 690,\n",
       " 'pass': 1276,\n",
       " 'wahrscheinlich': 1822,\n",
       " 'premier': 1320,\n",
       " 'steph': 1582,\n",
       " 'wahl': 1817,\n",
       " 'herzlich': 768,\n",
       " 'fes': 579,\n",
       " 'peter': 1287,\n",
       " 'verteidig': 1791,\n",
       " 'pro': 1326,\n",
       " 'international': 847,\n",
       " 'nsa': 1221,\n",
       " 'burg': 306,\n",
       " 'fen': 576,\n",
       " 'guck': 706,\n",
       " 'erhal': 510,\n",
       " 'erneu': 520,\n",
       " 'audi': 164,\n",
       " 'dat': 368,\n",
       " 'selb': 1492,\n",
       " 'doof': 422,\n",
       " 'johann': 881,\n",
       " 'knapp': 935,\n",
       " 'entwickel': 501,\n",
       " 'wann': 1826,\n",
       " 'lieg': 1022,\n",
       " '€': 1983,\n",
       " 'empfehl': 482,\n",
       " 'nachbar': 1152,\n",
       " 'warm': 1828,\n",
       " 'herz': 767,\n",
       " 'sprich': 1561,\n",
       " 'plotzlich': 1301,\n",
       " 'trai': 1679,\n",
       " 'find': 588,\n",
       " 'statt': 1573,\n",
       " 'rot': 1392,\n",
       " 'fahrlich': 553,\n",
       " 'laser': 991,\n",
       " 'freihei': 618,\n",
       " 'ang': 126,\n",
       " 'demo': 378,\n",
       " 'obwohl': 1233,\n",
       " 'darum': 367,\n",
       " 'respek': 1369,\n",
       " 'nam': 1162,\n",
       " 'anzieh': 140,\n",
       " 'wahrhei': 1821,\n",
       " 'follow': 598,\n",
       " 'me': 1075,\n",
       " 'bezahl': 244,\n",
       " 'artikel': 155,\n",
       " '★': 1986,\n",
       " 'geld': 660,\n",
       " 'verdie': 1754,\n",
       " 'blog': 270,\n",
       " 'iss': 858,\n",
       " 'krieg': 963,\n",
       " 'daum': 370,\n",
       " 'wofur': 1899,\n",
       " 'wohnung': 1902,\n",
       " 'partei': 1274,\n",
       " 'weit': 1849,\n",
       " 'letz': 1014,\n",
       " 'csu': 349,\n",
       " 'uberall': 1707,\n",
       " 'tha': 1651,\n",
       " 'e': 448,\n",
       " 'k': 891,\n",
       " 'trink': 1688,\n",
       " 'merkel': 1093,\n",
       " 'hach': 717,\n",
       " 'erklar': 515,\n",
       " 'witz': 1896,\n",
       " 'orfwahl': 1259,\n",
       " 'nee': 1173,\n",
       " 'klar': 924,\n",
       " 'opfer': 1255,\n",
       " '. .': 13,\n",
       " 'bild': 252,\n",
       " 'maratho': 1063,\n",
       " 'star': 1571,\n",
       " 'horror': 799,\n",
       " '´': 1972,\n",
       " 'lich': 1018,\n",
       " '♥': 1989,\n",
       " 'se': 1482,\n",
       " 'weh': 1842,\n",
       " 'altmaier': 116,\n",
       " 'polizei': 1311,\n",
       " 'yo': 1928,\n",
       " 'spar': 1546,\n",
       " 'bang': 195,\n",
       " 'moch': 1116,\n",
       " 'offiziell': 1240,\n",
       " 'abenteu': 84,\n",
       " 'attraktiv': 161,\n",
       " 'de': 375,\n",
       " 'look': 1033,\n",
       " 'bau': 198,\n",
       " 'not': 1215,\n",
       " 'audio': 165,\n",
       " 'htc': 805,\n",
       " 'beend': 206,\n",
       " 'anzeig': 139,\n",
       " 'hund': 811,\n",
       " '♡': 1988,\n",
       " 'herrlich': 765,\n",
       " 'shopping': 1507,\n",
       " 'her': 761,\n",
       " 'o_o': 1230,\n",
       " 'leich': 1004,\n",
       " 'meer': 1078,\n",
       " 'schmink': 1456,\n",
       " 'unternehm': 1729,\n",
       " 'kann': 900,\n",
       " 'ziemlich': 1946,\n",
       " 'droh': 438,\n",
       " 'wg': 1866,\n",
       " 'gg': 667,\n",
       " 'meis': 1084,\n",
       " 'nie': 1196,\n",
       " 'lau': 994,\n",
       " 'energie': 487,\n",
       " ':-d': 56,\n",
       " 'pic': 1292,\n",
       " 'team': 1633,\n",
       " 'sach': 1412,\n",
       " 'versicherung': 1782,\n",
       " 'fahrd': 552,\n",
       " 'schutz': 1474,\n",
       " 'irgendwie': 854,\n",
       " 'no': 1208,\n",
       " 'schoner': 1463,\n",
       " 'geb': 657,\n",
       " 'bes': 225,\n",
       " 'projek': 1332,\n",
       " 'biet': 250,\n",
       " 'ik': 826,\n",
       " 'ben': 215,\n",
       " 'oh': 1242,\n",
       " 'sorg': 1540,\n",
       " 'usa': 1741,\n",
       " 'scharf': 1435,\n",
       " 'kritik': 964,\n",
       " 'hi': 772,\n",
       " 'schaff': 1432,\n",
       " 'peinlich': 1281,\n",
       " 'todlich': 1669,\n",
       " 'ey': 546,\n",
       " 'trag': 1678,\n",
       " 'richtung': 1377,\n",
       " 'quatsch': 1336,\n",
       " 'robb': 1384,\n",
       " 'tu': 1695,\n",
       " ';': 62,\n",
       " '~': 1969,\n",
       " 'by': 311,\n",
       " 'unterstutz': 1732,\n",
       " 'dream': 431,\n",
       " 'produk': 1329,\n",
       " 'ruch': 1397,\n",
       " '16': 24,\n",
       " 'fuhrung': 636,\n",
       " 'cup': 350,\n",
       " 'bissl': 259,\n",
       " 'la': 978,\n",
       " 'usw': 1743,\n",
       " 'ad': 94,\n",
       " 'rum': 1404,\n",
       " 'och': 1234,\n",
       " 'low': 1038,\n",
       " 'hoffnung': 791,\n",
       " 'dfb': 399,\n",
       " 'bvb': 310,\n",
       " 'rein': 1362,\n",
       " 'wack': 1815,\n",
       " 'tra': 1676,\n",
       " 'wach': 1814,\n",
       " 'zweifel': 1965,\n",
       " 'koch': 938,\n",
       " '☺': 1987,\n",
       " 'trick': 1686,\n",
       " 'eige': 462,\n",
       " 'hasslich': 739,\n",
       " 'sorry': 1541,\n",
       " 'rau': 1346,\n",
       " 'nah': 1160,\n",
       " 'bet': 237,\n",
       " 'heh': 748,\n",
       " 'schlag': 1446,\n",
       " 'schuld': 1473,\n",
       " 'heinrich': 754,\n",
       " 'hang': 732,\n",
       " 'wand': 1825,\n",
       " 'ehrlich': 460,\n",
       " 'entscheidung': 498,\n",
       " 'half': 724,\n",
       " 'aktie': 104,\n",
       " 'org': 1260,\n",
       " 'gold': 682,\n",
       " 'schmuck': 1457,\n",
       " 'y': 1925,\n",
       " 'verspiel': 1784,\n",
       " 'r': 1338,\n",
       " 'l': 977,\n",
       " 'db': 374,\n",
       " 'bah': 190,\n",
       " 'munch': 1137,\n",
       " 'ander': 123,\n",
       " 'verbess': 1750,\n",
       " 'nachmittag': 1156,\n",
       " 'punk': 1334,\n",
       " 'komplett': 948,\n",
       " 'grau': 693,\n",
       " 'kaum': 907,\n",
       " 'fuer': 633,\n",
       " 'wtf': 1911,\n",
       " 'misch': 1108,\n",
       " 'verleih': 1767,\n",
       " '0': 16,\n",
       " 'pc': 1280,\n",
       " 'manch': 1060,\n",
       " 'liv': 1028,\n",
       " 'stream': 1595,\n",
       " 'anseh': 136,\n",
       " 'leagu': 997,\n",
       " 'feier': 574,\n",
       " 'bera': 218,\n",
       " 'mark': 1065,\n",
       " 'kommunikatio': 946,\n",
       " 'gmbh': 679,\n",
       " 'zurich': 1959,\n",
       " 'business': 309,\n",
       " 'besteh': 233,\n",
       " 'show': 1508,\n",
       " 'kommentar': 945,\n",
       " 'kopf': 954,\n",
       " 'schalk': 1434,\n",
       " 'entf': 494,\n",
       " ':-p': 57,\n",
       " 'paar': 1269,\n",
       " '{': 1966,\n",
       " '}': 1968,\n",
       " 'vergleich': 1759,\n",
       " 'mutt': 1145,\n",
       " 'madch': 1049,\n",
       " 'berg': 222,\n",
       " 'rug': 1401,\n",
       " 'ca': 315,\n",
       " 'hoch': 786,\n",
       " 'up': 1735,\n",
       " 'leb': 998,\n",
       " 'fashio': 565,\n",
       " 'week': 1840,\n",
       " 'pari': 1273,\n",
       " ':p': 61,\n",
       " 'hung': 812,\n",
       " 'tun': 1696,\n",
       " 'top': 1672,\n",
       " 'ei': 461,\n",
       " 'reduzier': 1357,\n",
       " 'pack': 1270,\n",
       " 'konz': 953,\n",
       " 'hoffentlich': 790,\n",
       " 'zuviel': 1963,\n",
       " 'gas': 655,\n",
       " 'himmel': 777,\n",
       " 'loh': 1030,\n",
       " 'saub': 1427,\n",
       " 'jung': 887,\n",
       " 'iler': 828,\n",
       " 'dia': 401,\n",
       " 'wild': 1877,\n",
       " 'tanz': 1627,\n",
       " 'genau': 662,\n",
       " 'min': 1101,\n",
       " '<': 67,\n",
       " 'ass': 158,\n",
       " 'window': 1883,\n",
       " 'bundesregierung': 304,\n",
       " 'beautiful': 203,\n",
       " 'beauty': 204,\n",
       " 'egal': 456,\n",
       " 'omg': 1248,\n",
       " 'ring': 1380,\n",
       " 'verei': 1756,\n",
       " 'schmeck': 1454,\n",
       " 'stree': 1596,\n",
       " 'food': 599,\n",
       " 'jug': 885,\n",
       " 'dri': 434,\n",
       " 'schwiegertochtergesuch': 1480,\n",
       " 'sg': 1501,\n",
       " 'chelsea': 332,\n",
       " 'fc': 569,\n",
       " 'official': 1239,\n",
       " 'bewer': 243,\n",
       " 'china': 335,\n",
       " 'bekann': 212,\n",
       " 'we': 1834,\n",
       " 'perfek': 1285,\n",
       " 'shop': 1505,\n",
       " 'boy': 285,\n",
       " 'eplay': 503,\n",
       " 'highligh': 774,\n",
       " 'titel': 1662,\n",
       " 'sturz': 1609,\n",
       " 'kanal': 898,\n",
       " 'schwierig': 1481,\n",
       " 'aufgab': 167,\n",
       " 'zahl': 1934,\n",
       " \"it'\": 861,\n",
       " 'never': 1190,\n",
       " 'rett': 1370,\n",
       " 'eis': 476,\n",
       " 'indie': 834,\n",
       " 'augu': 173,\n",
       " 'solar': 1532,\n",
       " 'engla': 490,\n",
       " 'bieber': 247,\n",
       " 'mtvhott': 1132,\n",
       " 'film': 586,\n",
       " 'apa': 142,\n",
       " 'beim': 209,\n",
       " 'bomb': 278,\n",
       " 'sexy': 1500,\n",
       " 'flieg': 594,\n",
       " 'erstmal': 527,\n",
       " 'halb': 723,\n",
       " 'club': 339,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val = pd.DataFrame(X_train[:2000])\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "#y_val = pd.DataFrame(y_train[:2000])\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.12833</td>\n",
       "      <td>0.365787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1     2         3     4         5     6     7     8     \\\n",
       "0  0.00000  0.000000   0.0  0.278921   0.0  0.286094   0.0   0.0   0.0   \n",
       "1  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "2  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "3  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "4  0.12833  0.365787   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "\n",
       "       9     ...  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3  0.129487  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4  0.108985  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reset_index(drop=True)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "import os\n",
    "data_dir = '../data/sentiment_data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.12833</td>\n",
       "      <td>0.365787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1         2     3         4     5         6     7     8     9     \\\n",
       "0     1  0.00000  0.000000   0.0  0.278921   0.0  0.286094   0.0   0.0   0.0   \n",
       "1     1  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "2     1  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "3     1  0.00000  0.000000   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "4     1  0.12833  0.365787   0.0  0.000000   0.0  0.000000   0.0   0.0   0.0   \n",
       "\n",
       "   ...  1991  1992  1993  1994  1995  1996  1997  1998  1999  2000  \n",
       "0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, save the test data to test.csv in the data_dir directory. Note that we do not save the associated ground truth\n",
    "# labels, instead we will use them later to compare with our model output.\n",
    "\n",
    "pd.DataFrame(X_test).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "# Save the training and validation data to train.csv and validation.csv in the data_dir directory.\n",
    "#pd.concat([y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "train_df = pd.concat([y_train, X_train], axis=1, ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "#X_test = X_train = X_val = y_train = y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Training files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment_data'\n",
    "\n",
    "# deleting bucket, uncomment lines below\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()\n",
    "\n",
    "# Upload the test.csv, train.csv and validation.csv files which are contained in data_dir to S3 using sess.upload_data().\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_data/test.csv\n",
      "sentiment_data/train.csv\n"
     ]
    }
   ],
   "source": [
    "empty_check = []\n",
    "\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your import and estimator code, here\n",
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is not the latest supported version. If you would like to use version 0.23-1, please add framework_version=0.23-1 to your constructor.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-20 09:11:08 Starting - Starting the training job...\n",
      "2020-08-20 09:11:09 Starting - Launching requested ML instances......\n",
      "2020-08-20 09:12:12 Starting - Preparing the instances for training...\n",
      "2020-08-20 09:13:01 Downloading - Downloading input data...\n",
      "2020-08-20 09:13:34 Training - Downloading the training image...\n",
      "2020-08-20 09:14:04 Uploading - Uploading generated training model\u001b[34m2020-08-20 09:13:54,760 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:54,763 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:54,775 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:55,046 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:55,046 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:55,046 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:55,046 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=5493 sha256=d39e19d77a83b78d836804281ec8bb15d3fa68fa771dc2ad8c289b8460c7736f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6kxxoalk/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:56,651 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:56,664 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-08-20-09-11-07-726\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-20-09-11-07-726/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-20-09-11-07-726/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-08-20-09-11-07-726\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-20-09-11-07-726/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[34m2020-08-20 09:13:59,984 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-20 09:14:11 Completed - Training job completed\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n",
      "CPU times: user 538 ms, sys: 21.4 ms, total: 559 ms\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# Train your estimator on S3 training data\n",
    "sklearn_estimator = SKLearn(entry_point='train.py',\n",
    "                            source_dir='source_sklearn',\n",
    "                            role=role,\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type='ml.m4.xlarge',\n",
    "                            sagemaker_session=session)\n",
    "sklearn_estimator.fit({'train': 's3://sagemaker-us-east-2-920979387335/sentiment_data/train.csv',\n",
    "                        'test': 's3://sagemaker-us-east-2-920979387335/sentiment_data/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 240 ms, sys: 17.1 ms, total: 258 ms\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = sklearn_estimator.deploy(instance_type='ml.m4.xlarge', #'ml.p2.xlarge',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# First: generate predicted, class labels\n",
    "y_test_preds = predictor.predict(X_test)\n",
    "\n",
    "# test that your model generates the correct number of labels\n",
    "assert len(y_test_preds)==len(y_test), 'Unexpected number of predictions.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "0.7675840978593272\n",
      "\n",
      "F1 macro score:\n",
      "0.6403065462588965\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.27      0.39        92\n",
      "           1       0.78      0.92      0.85       412\n",
      "           2       0.74      0.64      0.69       150\n",
      "\n",
      "    accuracy                           0.77       654\n",
      "   macro avg       0.73      0.61      0.64       654\n",
      "weighted avg       0.76      0.77      0.75       654\n",
      "\n",
      "\n",
      " Confusion matrix:\n",
      "[[0.27173913 0.61956522 0.10869565]\n",
      " [0.01699029 0.92475728 0.05825243]\n",
      " [0.03333333 0.32666667 0.64      ]]\n"
     ]
    }
   ],
   "source": [
    "# Second: calculate the test accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print('\\nAccuracy:')\n",
    "print(accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "print('\\nF1 macro score:')\n",
    "print(f1_score(y_test, y_test_preds, average='macro'))\n",
    "      \n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_test_preds))\n",
    "\n",
    "print('\\n Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_test_preds, normalize='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
