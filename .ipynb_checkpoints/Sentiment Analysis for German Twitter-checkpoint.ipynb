{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Sentiment Analysis of German Twitter\n",
    "\n",
    "This notebook is supposed to take a look at the data sued for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.\r\n",
      "  (use \"git pull\" to update your local branch)\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "\t\u001b[31mdeleted:    .ipynb_checkpoints/data_analysis-checkpoint.ipynb\u001b[m\r\n",
      "\t\u001b[31mdeleted:    data_analysis.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "\t\u001b[31m.ipynb_checkpoints/Sentiment Analysis for German Twitter-checkpoint.ipynb\u001b[m\r\n",
      "\t\u001b[31mSentiment Analysis for German Twitter.ipynb\u001b[m\r\n",
      "\t\u001b[31msource_sklearn/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m 'Adjusted preprocessing, changed classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"data/downloaded.tsv\", sep=\"\\t\", header=None, names=[\"id\", \"sentiment\", \"unknown1\", \"unknown2\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9939"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>unknown1</th>\n",
       "      <th>unknown2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367189542857482240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>c020aa23ff1f8ff985ce489b2b678674</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tainted Talents (Ateliertagebuch.) » Wir sind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>368327046574776321</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0096b66e311fffcca65c23d2a310083b</td>\n",
       "      <td>[]</td>\n",
       "      <td>Aber wenigstens kommt #Supernatural heute mal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>368309870673793024</td>\n",
       "      <td>neutral</td>\n",
       "      <td>575fd73efa41e07e2b0c360a721d19d7</td>\n",
       "      <td>[]</td>\n",
       "      <td>DARLEHEN - Angebot für Schufa-freie Darlehen: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>362896018389475328</td>\n",
       "      <td>neutral</td>\n",
       "      <td>8b824c765a7642a980b9e14c02830126</td>\n",
       "      <td>[]</td>\n",
       "      <td>ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>367912309303148545</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a07f6bc77b0cfb06a75c7cb1a88d752b</td>\n",
       "      <td>[]</td>\n",
       "      <td>Na? Wo sind Frankens heimliche Talente? - Die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id sentiment                          unknown1 unknown2  \\\n",
       "3  367189542857482240   neutral  c020aa23ff1f8ff985ce489b2b678674       []   \n",
       "4  368327046574776321   neutral  0096b66e311fffcca65c23d2a310083b       []   \n",
       "6  368309870673793024   neutral  575fd73efa41e07e2b0c360a721d19d7       []   \n",
       "7  362896018389475328   neutral  8b824c765a7642a980b9e14c02830126       []   \n",
       "8  367912309303148545   neutral  a07f6bc77b0cfb06a75c7cb1a88d752b       []   \n",
       "\n",
       "                                                text  \n",
       "3  Tainted Talents (Ateliertagebuch.) » Wir sind ...  \n",
       "4  Aber wenigstens kommt #Supernatural heute mal ...  \n",
       "6  DARLEHEN - Angebot für Schufa-freie Darlehen: ...  \n",
       "7  ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...  \n",
       "8  Na? Wo sind Frankens heimliche Talente? - Die ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = tweets_df[tweets_df['text'] != 'Not Available']\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6539"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f120a1a4470>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_df['sentiment'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "cleaned_df['sentiment'] = cleaned_df['sentiment'].apply(\n",
    "      lambda x: 2 if x == 'positive' else (0 if x == 'negative' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitting(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.2, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = splitting(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     Tainted Talents (Ateliertagebuch.) » Wir sind ...\n",
       "4     Aber wenigstens kommt #Supernatural heute mal ...\n",
       "6     DARLEHEN - Angebot für Schufa-freie Darlehen: ...\n",
       "7     ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...\n",
       "8     Na? Wo sind Frankens heimliche Talente? - Die ...\n",
       "9                        ... Glück breitet sich aus ...\n",
       "10                           @Moramee unachtsam *seufz*\n",
       "11    #jobs #Sales #Medien #TV #Tele M1 sucht #Persö...\n",
       "13    Happy Halloween liebe Studenten, zeigt eure gr...\n",
       "14    Die App „iDelete“ hat mir 3,4 MB belegten Spei...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem.cistem import Cistem\n",
    "\n",
    "stemmer = Cistem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(text):\n",
    "    text = text.lower() # Convert to lower case\n",
    "    text = re.sub(r\"http\\S+\", \"XXXURLTOKENXXX\", text) # replace links\n",
    "    text = re.sub(r\"@\\S+\", \"XXXUSERNAMETOKENXXX\", text) # replace mentionings with '@'\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text) # remove all numbers standing alone\n",
    "    text = re.sub(r'(?<=\\s)[^\\s\\w]*(?=[\\w])', '', text) # remove punctuations preceeding words\n",
    "    text = re.sub(r'[.]{2,}', '...', text) # unify consecutive periods if there are at least 2\n",
    "    text = re.sub(r'(?<=\\w)[^\\s\\w](?=[\\s\\w])', '', text) # remove single punctuation subceeding words (leaves emoticons)\n",
    "    text = re.sub( r'([\\w])([^\\s\\w])', r'\\1 \\2', text) # add white space between punctuations subceeding a word\n",
    "    text = re.sub(r'(?<=\\s)[^\\s\\w](?=[\\s])', '', text) # remove single punctuation preceeding empty space (leaves emoticons)\n",
    "    text = re.sub(r\"#\", \"\", text) # remove '#' in front of hashtags\n",
    "    text = re.sub(r\"rt \", \"\", text) # remove 'RT'\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"german\")] # Remove stopwords\n",
    "    words = [stemmer.stem(w) for w in words] # stem\n",
    "    \n",
    "    #from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi', 'ia', 'a', 'wonderful', 'tex', '!!', '77', '.7', 'seufz', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words('this ia a  . wonderful. text!! 55 77.7 *seufz.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_train, data_test):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # Preprocess training and test data to obtain words for each review\n",
    "    words_train = [review_to_words(review) for review in data_train]\n",
    "    words_test = [review_to_words(review) for review in data_test]\n",
    "    \n",
    "    return words_train, words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tainted', 'tal', 'ateliertagebuch', '.)', 'allei', 'xxxurltokenxxx'],\n",
       " ['wenig', 'komm', 'supernatural', 'heu', 'mal', 'uhr', 'schwach', 'tro'],\n",
       " ['darleh',\n",
       "  'angebo',\n",
       "  'schufa',\n",
       "  '-frei',\n",
       "  'darleh',\n",
       "  'gunstig',\n",
       "  'anbie',\n",
       "  'onli',\n",
       "  'darleh',\n",
       "  'deutschla',\n",
       "  'xxxurltokenxxx'],\n",
       " ['anruf',\n",
       "  'erwunsch',\n",
       "  'hardcor',\n",
       "  'teeny',\n",
       "  'vicky',\n",
       "  'carrera',\n",
       "  'hallo',\n",
       "  'suss',\n",
       "  'jahrig',\n",
       "  'vicky',\n",
       "  'deutschla',\n",
       "  '...',\n",
       "  'xxxurltokenxxx'],\n",
       " ['na',\n",
       "  'frank',\n",
       "  'heimlich',\n",
       "  'tal',\n",
       "  'ers',\n",
       "  'bewerbung',\n",
       "  'talentwettbewerb',\n",
       "  'wer',\n",
       "  'ko',\n",
       "  'darf',\n",
       "  '!\"...',\n",
       "  'xxxurltokenxxx'],\n",
       " ['...', 'gluck', 'brei', '...'],\n",
       " ['xxxusernametokenxxx', 'unachtsam', 'seufz', '*'],\n",
       " ['job',\n",
       "  'sal',\n",
       "  'medie',\n",
       "  'tv',\n",
       "  'tel',\n",
       "  'm1',\n",
       "  'such',\n",
       "  'personlichkei',\n",
       "  'werb',\n",
       "  'verkauf',\n",
       "  'aussendie',\n",
       "  'm',\n",
       "  '/w',\n",
       "  'xxxurltokenxxx'],\n",
       " ['happy', 'hallowee', 'lieb', 'stud', 'zeig', 'gruselig', 'seit', '!'],\n",
       " ['app',\n",
       "  'idel',\n",
       "  '3',\n",
       "  ',4',\n",
       "  'mb',\n",
       "  'beleg',\n",
       "  'speich',\n",
       "  'freigemach',\n",
       "  'losch',\n",
       "  'app',\n",
       "  'bring',\n",
       "  '16',\n",
       "  ',4',\n",
       "  'mb',\n",
       "  'frei',\n",
       "  'speich',\n",
       "  'tschuss',\n",
       "  '!'],\n",
       " ['xxxusernametokenxxx',\n",
       "  'moi',\n",
       "  '????',\n",
       "  'xxxusernametokenxxx',\n",
       "  'xxxusernametokenxxx',\n",
       "  'xxxusernametokenxxx',\n",
       "  'xxxusernametokenxxx'],\n",
       " ['check',\n",
       "  'out',\n",
       "  'the',\n",
       "  'new',\n",
       "  'pag',\n",
       "  'at',\n",
       "  'tim',\n",
       "  'abou',\n",
       "  'the',\n",
       "  'fir',\n",
       "  'world',\n",
       "  'schau',\n",
       "  'mal',\n",
       "  'neu',\n",
       "  'seit',\n",
       "  'ers',\n",
       "  '...',\n",
       "  'xxxurltokenxxx'],\n",
       " ['xxxusernametokenxxx', 'jop', 'agenda', 'andemuss', 'abend', 'ler', ':-\\\\'],\n",
       " ['grunderwerbsteu', 'erhoh', 'immobilienkauf', 'teur', 'xxxurltokenxxx'],\n",
       " ['gruppenliga',\n",
       "  'sga',\n",
       "  'porschmann',\n",
       "  'choteschovsky',\n",
       "  'dressbach',\n",
       "  'schneid',\n",
       "  'kor',\n",
       "  'siegfa',\n",
       "  'a',\n",
       "  'iger',\n",
       "  's',\n",
       "  'grochow',\n",
       "  'c',\n",
       "  'iger',\n",
       "  'mull',\n",
       "  'krau'],\n",
       " ['tal', 'tumblr', 'inter', 'allei', 'nerv', 'xxxurltokenxxx'],\n",
       " ['w',\n",
       "  'ieamp',\n",
       "  ';v',\n",
       "  'appl',\n",
       "  '-ev',\n",
       "  'viel',\n",
       "  'detail',\n",
       "  'sick',\n",
       "  'vorh',\n",
       "  'xxxurltokenxxx'],\n",
       " ['bad', 'endlich', 'wundervoll', 'xxxurltokenxxx'],\n",
       " ['xxxusernametokenxxx',\n",
       "  'hm',\n",
       "  '...',\n",
       "  '...?',\n",
       "  'mailadress',\n",
       "  'vielleich',\n",
       "  'schal',\n",
       "  'einfach',\n",
       "  ':)'],\n",
       " ['neu', 'nutzungsbedingung', 'googl', 'kurz', 'vorgestell', 'xxxurltokenxxx']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xxxusernametokenxxx', 3166), ('xxxurltokenxxx', 2447), ('...', 866), ('.', 542), ('!', 242), ('gut', 241), ('neu', 211), ('schon', 193), ('mehr', 179), ('?', 175), ('mach', 162), ('d', 140), ('geh', 133), ('mal', 132), ('dank', 127), ('…', 124), ('heu', 117), ('bitt', 106), ('einfach', 104), ('gross', 102), ('amp', 98), ('lieb', 96), ('ja', 96), ('klei', 95), (':)', 92), (\"'s\", 90), ('morg', 86), ('war', 85), ('ne', 83), ('komm', 81)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_words = get_all_words(X_train)\n",
    "\n",
    "\n",
    "freq_dist = FreqDist(all_words)\n",
    "print(freq_dist.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=2000):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "\n",
    "    # Fit a vectorizer to training documents and use it to transform them\n",
    "    # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "    #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "    vectorizer = TfidfVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "    features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "    # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "    features_test = vectorizer.transform(words_test).toarray()\n",
    "    \n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "X_train, X_test, vocabulary = extract_BoW_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tal': 1654,\n",
       " '.)': 60,\n",
       " 'allei': 209,\n",
       " 'xxxurltokenxxx': 1947,\n",
       " 'wenig': 1879,\n",
       " 'komm': 1014,\n",
       " 'heu': 847,\n",
       " 'mal': 1120,\n",
       " 'uhr': 1742,\n",
       " 'schwach': 1516,\n",
       " 'tro': 1716,\n",
       " 'darleh': 459,\n",
       " 'angebo': 225,\n",
       " 'schufa': 1511,\n",
       " 'gunstig': 789,\n",
       " 'anbie': 218,\n",
       " 'onli': 1298,\n",
       " 'deutschla': 484,\n",
       " 'anruf': 230,\n",
       " 'erwunsch': 603,\n",
       " 'hallo': 809,\n",
       " 'suss': 1647,\n",
       " '...': 62,\n",
       " 'na': 1205,\n",
       " 'frank': 685,\n",
       " 'heimlich': 832,\n",
       " 'ers': 599,\n",
       " 'wer': 1880,\n",
       " 'darf': 458,\n",
       " 'gluck': 755,\n",
       " 'brei': 390,\n",
       " 'xxxusernametokenxxx': 1948,\n",
       " 'seufz': 1540,\n",
       " '*': 26,\n",
       " 'job': 947,\n",
       " 'sal': 1454,\n",
       " 'medie': 1139,\n",
       " 'tv': 1729,\n",
       " 'tel': 1665,\n",
       " 'such': 1642,\n",
       " 'werb': 1881,\n",
       " 'verkauf': 1790,\n",
       " 'm': 1110,\n",
       " '/w': 91,\n",
       " 'happy': 819,\n",
       " 'hallowee': 810,\n",
       " 'lieb': 1084,\n",
       " 'stud': 1636,\n",
       " 'zeig': 1964,\n",
       " 'seit': 1531,\n",
       " '!': 0,\n",
       " 'app': 239,\n",
       " '3': 122,\n",
       " 'mb': 1135,\n",
       " 'speich': 1593,\n",
       " 'losch': 1100,\n",
       " 'bring': 392,\n",
       " '16': 106,\n",
       " 'frei': 693,\n",
       " 'tschuss': 1721,\n",
       " 'moi': 1181,\n",
       " '????': 173,\n",
       " 'check': 427,\n",
       " 'out': 1308,\n",
       " 'the': 1680,\n",
       " 'new': 1242,\n",
       " 'pag': 1315,\n",
       " 'at': 252,\n",
       " 'tim': 1685,\n",
       " 'fir': 664,\n",
       " 'world': 1932,\n",
       " 'schau': 1476,\n",
       " 'neu': 1238,\n",
       " 'abend': 179,\n",
       " 'ler': 1076,\n",
       " 'erhoh': 590,\n",
       " 'teur': 1674,\n",
       " 'schneid': 1500,\n",
       " 'a': 177,\n",
       " 's': 1446,\n",
       " 'c': 410,\n",
       " 'mull': 1192,\n",
       " 'tumblr': 1724,\n",
       " 'inter': 918,\n",
       " 'nerv': 1233,\n",
       " 'w': 1842,\n",
       " 'ieamp': 892,\n",
       " 'appl': 240,\n",
       " 'viel': 1823,\n",
       " 'vorh': 1837,\n",
       " 'bad': 281,\n",
       " 'endlich': 567,\n",
       " 'wundervoll': 1938,\n",
       " 'hm': 857,\n",
       " 'vielleich': 1824,\n",
       " 'schal': 1471,\n",
       " 'einfach': 547,\n",
       " ':)': 144,\n",
       " 'googl': 767,\n",
       " 'kurz': 1041,\n",
       " 'vorgestell': 1836,\n",
       " 'bereich': 323,\n",
       " 'personal': 1331,\n",
       " 'exper': 617,\n",
       " '…': 1994,\n",
       " 'berich': 325,\n",
       " 'spd': 1592,\n",
       " 'lass': 1058,\n",
       " 'gross': 780,\n",
       " 'koalitio': 1007,\n",
       " 'platz': 1340,\n",
       " 'schnell': 1501,\n",
       " 'of': 1284,\n",
       " 'fuck': 708,\n",
       " 'dow': 507,\n",
       " 'you': 1954,\n",
       " 'lik': 1090,\n",
       " 'sonn': 1580,\n",
       " 'spa': 1588,\n",
       " 'ess': 607,\n",
       " 'and': 219,\n",
       " 'lis': 1092,\n",
       " 'on': 1296,\n",
       " '?': 167,\n",
       " 'dach': 449,\n",
       " 'has': 822,\n",
       " 'ach': 187,\n",
       " 'ja': 938,\n",
       " 'dd': 468,\n",
       " 'bekomm': 315,\n",
       " 'ne': 1221,\n",
       " 'hoh': 867,\n",
       " 'max': 1133,\n",
       " '.': 58,\n",
       " 'frag': 684,\n",
       " 'ding': 495,\n",
       " 'schatt': 1474,\n",
       " 'hmmm': 859,\n",
       " 'fehl': 645,\n",
       " 'glaub': 752,\n",
       " 'falsch': 630,\n",
       " ':-)': 148,\n",
       " 'sich': 1550,\n",
       " 'druck': 520,\n",
       " 'web': 1863,\n",
       " 'beispiel': 310,\n",
       " 'roll': 1426,\n",
       " 'arbei': 241,\n",
       " 'sozial': 1587,\n",
       " 'amp': 217,\n",
       " 'immer': 904,\n",
       " 'mehr': 1142,\n",
       " 'mensch': 1152,\n",
       " 'ab': 178,\n",
       " 'wa': 1843,\n",
       " ':/': 150,\n",
       " 'geh': 736,\n",
       " 'fb': 641,\n",
       " 'manag': 1121,\n",
       " 'einsatz': 551,\n",
       " 'b': 278,\n",
       " 'schenk': 1481,\n",
       " 'zuletz': 1980,\n",
       " 'o': 1279,\n",
       " '.o': 82,\n",
       " 'voll': 1832,\n",
       " 'samstag': 1457,\n",
       " 'wel': 1874,\n",
       " 'via': 1821,\n",
       " 'offentlich': 1287,\n",
       " 'hand': 814,\n",
       " 'pos': 1355,\n",
       " 'grati': 772,\n",
       " '4': 130,\n",
       " 'winn': 1905,\n",
       " 'winnspiel': 1906,\n",
       " 'frisch': 703,\n",
       " 'gar': 731,\n",
       " 'ern': 594,\n",
       " 'herb': 840,\n",
       " 'herr': 841,\n",
       " 'kai': 958,\n",
       " 'mach': 1112,\n",
       " 'karrier': 968,\n",
       " 'leer': 1068,\n",
       " 'ick': 889,\n",
       " 'hoff': 863,\n",
       " 'fall': 629,\n",
       " 'verruck': 1803,\n",
       " 'sieh': 1555,\n",
       " 'zeit': 1965,\n",
       " 'berli': 326,\n",
       " 'seitensprung': 1532,\n",
       " 'wunsch': 1939,\n",
       " 'wahr': 1851,\n",
       " 'bitt': 360,\n",
       " 'nix': 1257,\n",
       " 'hab': 797,\n",
       " 'erreich': 598,\n",
       " 'beginn': 306,\n",
       " 'kenn': 976,\n",
       " 'accou': 186,\n",
       " 'much': 1191,\n",
       " 'okay': 1292,\n",
       " 'super': 1646,\n",
       " 'end': 566,\n",
       " '😉': 1998,\n",
       " 'wurd': 1940,\n",
       " 'grund': 784,\n",
       " 'snowd': 1570,\n",
       " 'dien': 491,\n",
       " 'benutz': 318,\n",
       " 'vergess': 1788,\n",
       " 'ganz': 730,\n",
       " 'tag': 1652,\n",
       " 'fahr': 624,\n",
       " 'sooo': 1583,\n",
       " 'gut': 790,\n",
       " 'warum': 1858,\n",
       " 'romantisch': 1428,\n",
       " 'empfehlung': 564,\n",
       " 'bald': 284,\n",
       " 'moglich': 1179,\n",
       " 'upda': 1770,\n",
       " '05': 96,\n",
       " 'if': 895,\n",
       " 'today': 1694,\n",
       " 'your': 1956,\n",
       " 'las': 1056,\n",
       " 'day': 466,\n",
       " 'interessa': 920,\n",
       " 'dabei': 448,\n",
       " 'konkr': 1019,\n",
       " 'perso': 1330,\n",
       " 'mein': 1143,\n",
       " 'offenbar': 1286,\n",
       " 'schein': 1479,\n",
       " 'davo': 464,\n",
       " 'mollath': 1182,\n",
       " 'helf': 837,\n",
       " 'to': 1692,\n",
       " 'sleep': 1564,\n",
       " 'nach': 1206,\n",
       " 'well': 1875,\n",
       " 'ech': 534,\n",
       " 'battlestar': 291,\n",
       " 'galactica': 724,\n",
       " 'wett': 1886,\n",
       " '.xxxusernametokenxxx': 83,\n",
       " 'serie': 1537,\n",
       " 'brauch': 388,\n",
       " 'osterreich': 1306,\n",
       " 'auftrag': 262,\n",
       " 'politik': 1349,\n",
       " 'frau': 690,\n",
       " 'kaputt': 966,\n",
       " 'unbeding': 1749,\n",
       " 'richtig': 1417,\n",
       " 'd': 447,\n",
       " 'facebook': 622,\n",
       " 'unglucklich': 1755,\n",
       " 'konn': 1020,\n",
       " 'bestatig': 334,\n",
       " 'i': 886,\n",
       " 'com': 439,\n",
       " 'leipzig': 1073,\n",
       " 'nein': 1228,\n",
       " 'politisch': 1350,\n",
       " 'bleib': 365,\n",
       " 'meld': 1148,\n",
       " 'morg': 1187,\n",
       " 'fruh': 706,\n",
       " 'los': 1099,\n",
       " 'spiel': 1596,\n",
       " 'nochmal': 1260,\n",
       " 'ok': 1291,\n",
       " 'dank': 454,\n",
       " 'steh': 1615,\n",
       " 'drauss': 512,\n",
       " 'schon': 1503,\n",
       " 'novemb': 1268,\n",
       " 'mag': 1115,\n",
       " 'ster': 1620,\n",
       " 'notig': 1267,\n",
       " 'genug': 741,\n",
       " 'schrott': 1510,\n",
       " 'unmoglich': 1758,\n",
       " ';)': 161,\n",
       " 'buch': 396,\n",
       " 'mi': 1159,\n",
       " '18': 108,\n",
       " '.09': 71,\n",
       " '.13': 74,\n",
       " 'lang': 1052,\n",
       " 'intensiv': 917,\n",
       " 'traum': 1707,\n",
       " 'schlaf': 1485,\n",
       " 'ho': 860,\n",
       " 'seh': 1528,\n",
       " 'erfolgreich': 586,\n",
       " 'air': 200,\n",
       " 'jema': 945,\n",
       " 'tipp': 1687,\n",
       " 'neuer': 1239,\n",
       " 'lahm': 1050,\n",
       " 'boateng': 374,\n",
       " 'khedira': 981,\n",
       " 'ozil': 1311,\n",
       " 'klo': 1002,\n",
       " 'mog': 1178,\n",
       " 'verstarkung': 1811,\n",
       " 'us': 1775,\n",
       " 'justi': 954,\n",
       " 'jahr': 940,\n",
       " 'wohl': 1924,\n",
       " 'sportlich': 1600,\n",
       " 'gonn': 765,\n",
       " 'termi': 1670,\n",
       " 'pau': 1321,\n",
       " 'gabriel': 723,\n",
       " 'kind': 986,\n",
       " 'soll': 1576,\n",
       " 'hilf': 852,\n",
       " 'erledig': 593,\n",
       " 'regierung': 1400,\n",
       " 'sel': 1533,\n",
       " 'sundhei': 1645,\n",
       " 'le': 1062,\n",
       " 'vintag': 1826,\n",
       " 'aktuell': 204,\n",
       " 'klei': 999,\n",
       " 'pho': 1334,\n",
       " '49': 131,\n",
       " 'now': 1269,\n",
       " 'or': 1302,\n",
       " '“xxxusernametokenxxx': 1993,\n",
       " 'scheiss': 1480,\n",
       " 'drauf': 511,\n",
       " 'anfang': 222,\n",
       " 'nutz': 1278,\n",
       " 'is': 932,\n",
       " 'lieber': 1085,\n",
       " 'teuer': 1673,\n",
       " 'aufgeb': 260,\n",
       " ';-)': 162,\n",
       " 'ist': 933,\n",
       " 'n': 1203,\n",
       " 'versuch': 1813,\n",
       " '17': 107,\n",
       " 'positiv': 1356,\n",
       " 'denk': 474,\n",
       " '8': 138,\n",
       " 'arm': 245,\n",
       " 'baby': 279,\n",
       " 'klau': 998,\n",
       " 'hamburg': 811,\n",
       " 'pla': 1338,\n",
       " 'autor': 275,\n",
       " 'sitz': 1563,\n",
       " 'schreib': 1505,\n",
       " 'unsich': 1761,\n",
       " 'lach': 1046,\n",
       " 'ha': 795,\n",
       " 'ebe': 531,\n",
       " 'innovatio': 916,\n",
       " 'blau': 364,\n",
       " 'danach': 453,\n",
       " 'back': 280,\n",
       " 'sonntag': 1581,\n",
       " 'mona': 1184,\n",
       " 'eigentlich': 544,\n",
       " 'merk': 1155,\n",
       " '0': 92,\n",
       " '.5': 78,\n",
       " '.8': 79,\n",
       " 'off': 1285,\n",
       " 'desig': 481,\n",
       " 'schaf': 1469,\n",
       " 'my': 1202,\n",
       " 'witzig': 1919,\n",
       " 'link': 1091,\n",
       " 'folg': 674,\n",
       " 'hat': 825,\n",
       " 'kill': 984,\n",
       " 'be': 295,\n",
       " 'band': 286,\n",
       " 'musik': 1197,\n",
       " 'metal': 1158,\n",
       " 'freu': 698,\n",
       " 'idio': 891,\n",
       " 'kauf': 972,\n",
       " 'hahah': 801,\n",
       " 'deutlich': 482,\n",
       " 'bor': 380,\n",
       " 'freundi': 700,\n",
       " 'nich': 1245,\n",
       " '2': 110,\n",
       " '-0': 42,\n",
       " 'stark': 1611,\n",
       " '!!!': 2,\n",
       " 'befurch': 305,\n",
       " 'hoher': 868,\n",
       " 'weg': 1867,\n",
       " 'beid': 307,\n",
       " 'spo': 1598,\n",
       " 'math': 1131,\n",
       " 'hey': 848,\n",
       " 'kostenlo': 1029,\n",
       " 'sex': 1541,\n",
       " 'cha': 421,\n",
       " 'dieb': 490,\n",
       " 'aktiv': 203,\n",
       " 'englisch': 572,\n",
       " 'ball': 285,\n",
       " 'heiss': 835,\n",
       " 'wuss': 1941,\n",
       " 'gomez': 764,\n",
       " 'dienstag': 492,\n",
       " '.2013': 76,\n",
       " '19': 109,\n",
       " ':30': 156,\n",
       " 'wiener': 1896,\n",
       " 'hall': 808,\n",
       " 'f': 620,\n",
       " 'wien': 1895,\n",
       " 'eve': 613,\n",
       " 'toll': 1696,\n",
       " 'lt': 1104,\n",
       " ';3': 164,\n",
       " ':--': 149,\n",
       " 'schlimm': 1494,\n",
       " 'langsam': 1054,\n",
       " 'cdu': 419,\n",
       " 'girl': 750,\n",
       " 'geil': 737,\n",
       " 'nett': 1236,\n",
       " 'troff': 1717,\n",
       " 'foto': 682,\n",
       " 'alter': 212,\n",
       " 'krank': 1031,\n",
       " '),': 23,\n",
       " 'inkl': 914,\n",
       " '.:': 80,\n",
       " 'fas': 637,\n",
       " 'fit': 666,\n",
       " 'bar': 289,\n",
       " 'review': 1413,\n",
       " 'apfel': 238,\n",
       " '/': 84,\n",
       " 'ost': 1305,\n",
       " 'kevi': 979,\n",
       " 'ger': 743,\n",
       " '1': 99,\n",
       " ':0': 151,\n",
       " 'hertha': 843,\n",
       " 'bsc': 394,\n",
       " 'eintrach': 554,\n",
       " 'frankfu': 686,\n",
       " 'don': 504,\n",
       " 'cool': 443,\n",
       " 'nimm': 1255,\n",
       " 'big': 352,\n",
       " 'keith': 975,\n",
       " '25': 117,\n",
       " 'vertrag': 1815,\n",
       " 'boah': 373,\n",
       " 'nd': 1219,\n",
       " 'sag': 1451,\n",
       " 'dumm': 522,\n",
       " 'auf': 257,\n",
       " 'hass': 823,\n",
       " 'unbekann': 1750,\n",
       " 'forderung': 681,\n",
       " 'meldung': 1149,\n",
       " 'deutsch': 483,\n",
       " '2013': 112,\n",
       " 'rad': 1383,\n",
       " 'sech': 1525,\n",
       " 'angenehm': 227,\n",
       " 'wahlkampf': 1849,\n",
       " 'spass': 1591,\n",
       " 'fdp': 644,\n",
       " 'weltbild': 1876,\n",
       " 'arsch': 246,\n",
       " 'lustig': 1108,\n",
       " 'plaka': 1339,\n",
       " '?!': 168,\n",
       " 'bundesliga': 400,\n",
       " 'volk': 1831,\n",
       " 'stopp': 1624,\n",
       " 'somm': 1577,\n",
       " 'vorbei': 1834,\n",
       " 'sinn': 1561,\n",
       " 'hal': 805,\n",
       " 'fress': 697,\n",
       " 'leg': 1069,\n",
       " 'tut': 1727,\n",
       " 'video': 1822,\n",
       " \":'(\": 141,\n",
       " 'durf': 525,\n",
       " 'real': 1394,\n",
       " 'verlass': 1792,\n",
       " 'birthday': 357,\n",
       " 'tier': 1683,\n",
       " 'drog': 518,\n",
       " 'waff': 1846,\n",
       " 'bestell': 336,\n",
       " 'gleich': 753,\n",
       " 'aufschrei': 261,\n",
       " 'tot': 1700,\n",
       " 'punktlich': 1376,\n",
       " 'wunderscho': 1937,\n",
       " 'kredi': 1033,\n",
       " 'aktio': 202,\n",
       " '-.-': 40,\n",
       " 'war': 1856,\n",
       " 'darauf': 457,\n",
       " 'chanc': 424,\n",
       " 'schlech': 1491,\n",
       " 'ahnung': 199,\n",
       " 'mutti': 1201,\n",
       " 'wird': 1907,\n",
       " 'rich': 1416,\n",
       " 'deshalb': 480,\n",
       " 'gru': 783,\n",
       " 'naturlich': 1216,\n",
       " 'afd': 190,\n",
       " 'fussball': 720,\n",
       " 'stuttgar': 1641,\n",
       " 'kick': 982,\n",
       " 'calvi': 412,\n",
       " 'men': 1150,\n",
       " \"'s\": 18,\n",
       " 'x': 1943,\n",
       " 'extr': 618,\n",
       " 'slim': 1565,\n",
       " '20': 111,\n",
       " 'schad': 1468,\n",
       " 'bayer': 294,\n",
       " 'leverku': 1081,\n",
       " 'ubrig': 1740,\n",
       " 'nintendo': 1256,\n",
       " '2ds': 121,\n",
       " 'zwei': 1985,\n",
       " '(!)': 19,\n",
       " 'bedeu': 301,\n",
       " 'fal': 628,\n",
       " 'leistung': 1075,\n",
       " ':(': 143,\n",
       " 'sprech': 1602,\n",
       " 'dick': 488,\n",
       " 'quo': 1381,\n",
       " 'wieso': 1898,\n",
       " '\"?': 13,\n",
       " 'pira': 1337,\n",
       " 'soo': 1582,\n",
       " 'bess': 332,\n",
       " \"'m\": 17,\n",
       " 'going': 762,\n",
       " 'lauf': 1061,\n",
       " 'gott': 768,\n",
       " 'stimm': 1622,\n",
       " 'sund': 1644,\n",
       " '?\"': 169,\n",
       " 'ruck': 1436,\n",
       " '.000': 68,\n",
       " 'je': 943,\n",
       " 'eur': 610,\n",
       " 'amazo': 216,\n",
       " 'luf': 1105,\n",
       " 'zieh': 1968,\n",
       " 'hopp': 872,\n",
       " 'mann': 1124,\n",
       " 'reich': 1402,\n",
       " 'bock': 376,\n",
       " 'tiger': 1684,\n",
       " 'sowa': 1586,\n",
       " 'chef': 428,\n",
       " 'muss': 1198,\n",
       " 'beschw': 329,\n",
       " 'u': 1735,\n",
       " 'fang': 634,\n",
       " '***': 27,\n",
       " 'inhal': 913,\n",
       " 'urteil': 1774,\n",
       " 'erfolg': 585,\n",
       " 'har': 820,\n",
       " 'tft': 1677,\n",
       " '21': 113,\n",
       " ',5': 37,\n",
       " 'zoll': 1973,\n",
       " 'led': 1066,\n",
       " 'weiss': 1870,\n",
       " 'leider': 1072,\n",
       " 'schick': 1483,\n",
       " '250': 118,\n",
       " ',-': 33,\n",
       " 'euro': 611,\n",
       " 'schweiz': 1519,\n",
       " 'bea': 296,\n",
       " 'journali': 949,\n",
       " 'rech': 1395,\n",
       " 'energiew': 569,\n",
       " 'steck': 1614,\n",
       " 'tru': 1720,\n",
       " 'lov': 1102,\n",
       " 'fun': 715,\n",
       " 'kiss': 990,\n",
       " 'win': 1902,\n",
       " 'kal': 959,\n",
       " 'bla': 362,\n",
       " 'lachel': 1047,\n",
       " 'besserung': 333,\n",
       " 'get': 744,\n",
       " 'gladbach': 751,\n",
       " 'ile': 900,\n",
       " 'fuhl': 710,\n",
       " 'krass': 1032,\n",
       " 'gluckwunsch': 757,\n",
       " \":')\": 142,\n",
       " 'porsch': 1354,\n",
       " 'alt': 211,\n",
       " 'spann': 1589,\n",
       " 'numm': 1276,\n",
       " 'kri': 1034,\n",
       " 'kim': 985,\n",
       " 'photo': 1335,\n",
       " 'westfalisch': 1885,\n",
       " 'bielefeld': 349,\n",
       " '):': 25,\n",
       " 'schwer': 1520,\n",
       " 'aussich': 272,\n",
       " 'komisch': 1013,\n",
       " 'farb': 636,\n",
       " 'entspann': 579,\n",
       " 'aug': 264,\n",
       " 'studie': 1637,\n",
       " 'kita': 991,\n",
       " 'einstei': 552,\n",
       " ',,': 31,\n",
       " 'weiterhi': 1873,\n",
       " 'hol': 869,\n",
       " 'hor': 873,\n",
       " 'hamm': 812,\n",
       " 'bot': 383,\n",
       " 'stell': 1619,\n",
       " 'zuruck': 1983,\n",
       " '!!': 1,\n",
       " 'uber': 1736,\n",
       " 'handy': 816,\n",
       " 'rechnung': 1396,\n",
       " 'verdamm': 1785,\n",
       " 'aha': 197,\n",
       " 'les': 1077,\n",
       " 'ever': 614,\n",
       " 'unterschied': 1764,\n",
       " 'good': 766,\n",
       " 'nic': 1244,\n",
       " 'nauso': 1217,\n",
       " 'moder': 1177,\n",
       " '^^': 175,\n",
       " 'frech': 691,\n",
       " 'hubsch': 879,\n",
       " 'montag': 1185,\n",
       " 'per': 1328,\n",
       " 'ausserd': 271,\n",
       " 'all': 208,\n",
       " 'exklusiv': 616,\n",
       " 'klassisch': 997,\n",
       " 'hau': 827,\n",
       " 'wiss': 1914,\n",
       " 'mon': 1183,\n",
       " 'streich': 1631,\n",
       " 'mis': 1169,\n",
       " 'drei': 514,\n",
       " 'dankescho': 455,\n",
       " 'ff': 658,\n",
       " 'gruss': 785,\n",
       " 'full': 714,\n",
       " 'furch': 719,\n",
       " 'grad': 771,\n",
       " 'oz': 1310,\n",
       " 'pass': 1320,\n",
       " 'wahrscheinlich': 1853,\n",
       " 'premier': 1360,\n",
       " 'wahl': 1848,\n",
       " 'herzlich': 845,\n",
       " 'fes': 653,\n",
       " 'peter': 1333,\n",
       " 'verteidig': 1814,\n",
       " 'bl': 361,\n",
       " 'pro': 1365,\n",
       " 'international': 921,\n",
       " 'franzosisch': 689,\n",
       " 'nsa': 1273,\n",
       " 'burg': 404,\n",
       " 'ielt': 894,\n",
       " 'fen': 650,\n",
       " 'guck': 787,\n",
       " 'erhal': 589,\n",
       " 'erneu': 596,\n",
       " 'bmw': 372,\n",
       " 'audi': 256,\n",
       " 'vorfreud': 1835,\n",
       " 'dat': 461,\n",
       " 'selb': 1534,\n",
       " 'doof': 505,\n",
       " '+++': 30,\n",
       " 'frankfur': 687,\n",
       " 'johann': 948,\n",
       " 'knapp': 1005,\n",
       " 'entwickel': 581,\n",
       " 'wann': 1855,\n",
       " 'lieg': 1087,\n",
       " 'empfehl': 563,\n",
       " 'notebook': 1266,\n",
       " 'pol': 1348,\n",
       " 'nachbar': 1207,\n",
       " 'warm': 1857,\n",
       " 'herz': 844,\n",
       " 'sprich': 1603,\n",
       " 'plotzlich': 1343,\n",
       " 'trai': 1704,\n",
       " 'find': 663,\n",
       " 'statt': 1612,\n",
       " 'rot': 1431,\n",
       " 'fahrlich': 626,\n",
       " 'laser': 1057,\n",
       " 'freihei': 695,\n",
       " 'ang': 223,\n",
       " 'demo': 471,\n",
       " 'obwohl': 1283,\n",
       " 'darum': 460,\n",
       " 'respek': 1410,\n",
       " 'nam': 1213,\n",
       " 't': 1651,\n",
       " 'anzieh': 236,\n",
       " 'wahrhei': 1852,\n",
       " 'follow': 675,\n",
       " 'me': 1137,\n",
       " 'bezahl': 345,\n",
       " 'artikel': 247,\n",
       " 'geld': 738,\n",
       " 'verdie': 1786,\n",
       " 'blog': 370,\n",
       " '...u': 65,\n",
       " 'krieg': 1035,\n",
       " 'daum': 463,\n",
       " 'wofur': 1922,\n",
       " 'wohnung': 1925,\n",
       " 'partei': 1318,\n",
       " 'weit': 1871,\n",
       " 'letz': 1079,\n",
       " 'uberall': 1737,\n",
       " 'tha': 1678,\n",
       " 'e': 529,\n",
       " 'k': 955,\n",
       " 'trink': 1714,\n",
       " 'merkel': 1156,\n",
       " 'peni': 1326,\n",
       " 'hach': 799,\n",
       " 'erklar': 592,\n",
       " 'witz': 1918,\n",
       " 'nee': 1223,\n",
       " 'klar': 994,\n",
       " 'bild': 353,\n",
       " 'maratho': 1126,\n",
       " 'star': 1610,\n",
       " 'horror': 874,\n",
       " '´s': 1990,\n",
       " 'lich': 1083,\n",
       " '♥': 1996,\n",
       " 'se': 1524,\n",
       " 'weh': 1868,\n",
       " 'altmaier': 213,\n",
       " 'polizei': 1351,\n",
       " 'yo': 1952,\n",
       " 'need': 1224,\n",
       " 'eu': 609,\n",
       " 'spar': 1590,\n",
       " '100': 101,\n",
       " 'bang': 287,\n",
       " 'moch': 1176,\n",
       " 'abenteu': 180,\n",
       " 'attraktiv': 253,\n",
       " 'de': 469,\n",
       " 'look': 1098,\n",
       " 'bau': 292,\n",
       " 'htc': 878,\n",
       " 'beend': 303,\n",
       " 'anzeig': 235,\n",
       " 'hund': 884,\n",
       " 'herrlich': 842,\n",
       " 'her': 839,\n",
       " 'o_o': 1280,\n",
       " 'leich': 1070,\n",
       " 'meer': 1140,\n",
       " 'schmink': 1497,\n",
       " 'unternehm': 1763,\n",
       " 'kann': 964,\n",
       " 'ziemlich': 1970,\n",
       " 'droh': 519,\n",
       " 'wg': 1887,\n",
       " 'gg': 745,\n",
       " 'meis': 1146,\n",
       " 'nie': 1247,\n",
       " 'schauspiel': 1478,\n",
       " 'lau': 1060,\n",
       " 'energie': 568,\n",
       " 'pic': 1336,\n",
       " 'team': 1660,\n",
       " 'sach': 1450,\n",
       " 'fahrd': 625,\n",
       " 'schutz': 1515,\n",
       " 'irgendwie': 930,\n",
       " 'no': 1259,\n",
       " 'twee': 1732,\n",
       " 'schoner': 1504,\n",
       " 'geb': 735,\n",
       " 'bes': 327,\n",
       " 'projek': 1371,\n",
       " 'biet': 351,\n",
       " 'ik': 899,\n",
       " 'ben': 317,\n",
       " 'oh': 1290,\n",
       " 'god': 761,\n",
       " '.de': 81,\n",
       " 'sorg': 1584,\n",
       " 'usa': 1776,\n",
       " 'scharf': 1473,\n",
       " 'kritik': 1036,\n",
       " 'hi': 849,\n",
       " 'schaff': 1470,\n",
       " 'peinlich': 1325,\n",
       " 'todlich': 1695,\n",
       " 'ey': 619,\n",
       " 'trag': 1703,\n",
       " '?:': 170,\n",
       " 'richtung': 1418,\n",
       " '30': 123,\n",
       " '.08': 70,\n",
       " 'quatsch': 1380,\n",
       " 'robb': 1424,\n",
       " 'tu': 1723,\n",
       " '7': 136,\n",
       " 'by': 408,\n",
       " 'unterstutz': 1766,\n",
       " 'produk': 1368,\n",
       " 'ruch': 1435,\n",
       " '-jahrig': 49,\n",
       " 'sturm': 1639,\n",
       " 'fuhrung': 712,\n",
       " 'cup': 446,\n",
       " 'la': 1045,\n",
       " 'usw': 1777,\n",
       " 'twitt': 1733,\n",
       " 'ad': 189,\n",
       " 'low': 1103,\n",
       " 'hoffnung': 866,\n",
       " 'dfb': 486,\n",
       " 'bvb': 407,\n",
       " ')': 22,\n",
       " 'rein': 1403,\n",
       " ':))': 145,\n",
       " 'wack': 1845,\n",
       " 'tra': 1702,\n",
       " 'wach': 1844,\n",
       " 'zweifel': 1986,\n",
       " 'koch': 1008,\n",
       " 'trick': 1712,\n",
       " 'eige': 543,\n",
       " 'luxu': 1109,\n",
       " '-video': 57,\n",
       " 'hasslich': 824,\n",
       " 'sorry': 1585,\n",
       " 'rau': 1390,\n",
       " 'nah': 1211,\n",
       " 'heh': 830,\n",
       " 'schlag': 1486,\n",
       " 'gold': 763,\n",
       " 'schuld': 1514,\n",
       " 'heinrich': 833,\n",
       " 'hang': 817,\n",
       " 'ehrlich': 541,\n",
       " 'entscheidung': 578,\n",
       " 'half': 807,\n",
       " 'aktie': 201,\n",
       " 'trenn': 1711,\n",
       " 'schmuck': 1498,\n",
       " 'y': 1949,\n",
       " 'verspiel': 1808,\n",
       " 'l': 1044,\n",
       " 'db': 467,\n",
       " 'bah': 282,\n",
       " 'munch': 1194,\n",
       " 'ander': 220,\n",
       " 'verbess': 1782,\n",
       " 'nachmittag': 1208,\n",
       " 'punk': 1375,\n",
       " 'komplett': 1017,\n",
       " 'grau': 774,\n",
       " 'haha': 800,\n",
       " 'kaum': 973,\n",
       " 'fuer': 709,\n",
       " 'wtf': 1934,\n",
       " 'verleih': 1793,\n",
       " 'manch': 1122,\n",
       " 'liv': 1093,\n",
       " 'stream': 1628,\n",
       " '29': 120,\n",
       " '.10': 73,\n",
       " 'football': 677,\n",
       " 'leagu': 1063,\n",
       " 'feier': 647,\n",
       " '!\"': 4,\n",
       " 'bera': 320,\n",
       " '/in': 89,\n",
       " 'mark': 1128,\n",
       " 'gmbh': 758,\n",
       " 'zurich': 1982,\n",
       " 'besteh': 335,\n",
       " 'show': 1549,\n",
       " 'kommentar': 1015,\n",
       " 'kopf': 1024,\n",
       " 'schalk': 1472,\n",
       " 'entf': 574,\n",
       " 'p': 1312,\n",
       " 'paar': 1313,\n",
       " '{}': 1987,\n",
       " 'vergleich': 1789,\n",
       " 'mutt': 1200,\n",
       " 'madch': 1114,\n",
       " 'berg': 324,\n",
       " 'rug': 1439,\n",
       " 'hoch': 861,\n",
       " 'up': 1769,\n",
       " 'leb': 1064,\n",
       " 'fashio': 638,\n",
       " 'week': 1866,\n",
       " 'pari': 1317,\n",
       " 'hung': 885,\n",
       " 'tun': 1725,\n",
       " 'top': 1698,\n",
       " '70': 137,\n",
       " '???': 172,\n",
       " 'pack': 1314,\n",
       " 'konz': 1022,\n",
       " 'hoffentlich': 865,\n",
       " 'gas': 732,\n",
       " 'himmel': 853,\n",
       " 'loh': 1095,\n",
       " 'saub': 1465,\n",
       " 'jung': 952,\n",
       " 'iler': 901,\n",
       " '22': 114,\n",
       " '.00': 67,\n",
       " 'dunkl': 524,\n",
       " 'dia': 487,\n",
       " '...!': 63,\n",
       " 'wild': 1899,\n",
       " 'tanz': 1655,\n",
       " 'boom': 379,\n",
       " 'genau': 740,\n",
       " 'min': 1165,\n",
       " ';iel': 166,\n",
       " 'ass': 250,\n",
       " 'window': 1904,\n",
       " 'bundesregierung': 401,\n",
       " 'beautiful': 298,\n",
       " 'beauty': 299,\n",
       " '-chef': 48,\n",
       " 'egal': 537,\n",
       " 'omg': 1295,\n",
       " 'ring': 1421,\n",
       " 'verei': 1787,\n",
       " 'schmeck': 1495,\n",
       " 'stree': 1629,\n",
       " 'food': 676,\n",
       " 'jug': 950,\n",
       " 'dri': 515,\n",
       " 'sg': 1544,\n",
       " 'keep': 974,\n",
       " 'kanzleri': 965,\n",
       " 'chelsea': 429,\n",
       " 'fc': 642,\n",
       " 'official': 1288,\n",
       " 'bewer': 344,\n",
       " '-tv': 56,\n",
       " 'china': 432,\n",
       " 'bekann': 314,\n",
       " 'we': 1862,\n",
       " 'perfek': 1329,\n",
       " '.,': 61,\n",
       " 'pet': 1332,\n",
       " 'shop': 1548,\n",
       " 'boy': 385,\n",
       " 'eplay': 582,\n",
       " 'highligh': 851,\n",
       " 'titel': 1688,\n",
       " 'wm': 1920,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val = pd.DataFrame(X_train[:2000])\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "#y_val = pd.DataFrame(y_train[:2000])\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1990  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "   1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reset_index(drop=True)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "data_dir = '../data/sentiment_data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1991  \\\n",
       "0     1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1     1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2     1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3     1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4     1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "   1992  1993  1994  1995  1996  1997  1998  1999  2000  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, save the test data to test.csv in the data_dir directory. Note that we do not save the associated ground truth\n",
    "# labels, instead we will use them later to compare with our model output.\n",
    "\n",
    "pd.DataFrame(X_test).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "# Save the training and validation data to train.csv and validation.csv in the data_dir directory.\n",
    "#pd.concat([y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "train_df = pd.concat([y_train, X_train], axis=1, ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
    "#X_test = X_train = X_val = y_train = y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Training / Validation files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment_data'\n",
    "\n",
    "# deleting bucket, uncomment lines below\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()\n",
    "\n",
    "# Upload the test.csv, train.csv and validation.csv files which are contained in data_dir to S3 using sess.upload_data().\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_data/test.csv\n",
      "sentiment_data/train.csv\n"
     ]
    }
   ],
   "source": [
    "empty_check = []\n",
    "\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your import and estimator code, here\n",
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is not the latest supported version. If you would like to use version 0.23-1, please add framework_version=0.23-1 to your constructor.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-14 16:14:22 Starting - Starting the training job...\n",
      "2020-08-14 16:14:24 Starting - Launching requested ML instances......\n",
      "2020-08-14 16:15:28 Starting - Preparing the instances for training...\n",
      "2020-08-14 16:16:21 Downloading - Downloading input data...\n",
      "2020-08-14 16:16:36 Training - Downloading the training image..\u001b[34m2020-08-14 16:16:56,160 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,164 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,183 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,442 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,442 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,442 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:56,442 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=5851 sha256=151db72960b51750c9e9903f0da005c155e3ccb3b36d3e767b50226f1c1fd471\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hg74oz6x/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:57,870 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-14 16:16:57,883 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-08-14-16-14-22-654\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-14-16-14-22-654/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-14-16-14-22-654/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-08-14-16-14-22-654\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-920979387335/sagemaker-scikit-learn-2020-08-14-16-14-22-654/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[34m2020-08-14 16:17:00,903 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-14 16:17:08 Uploading - Uploading generated training model\n",
      "2020-08-14 16:17:08 Completed - Training job completed\n",
      "Training seconds: 47\n",
      "Billable seconds: 47\n",
      "CPU times: user 449 ms, sys: 37.3 ms, total: 487 ms\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# Train your estimator on S3 training data\n",
    "sklearn_estimator = SKLearn(entry_point='train.py',\n",
    "                            source_dir='source_sklearn',\n",
    "                            role=role,\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type='ml.m4.xlarge',\n",
    "                            sagemaker_session=session)\n",
    "sklearn_estimator.fit({'train': 's3://sagemaker-us-east-2-920979387335/sentiment_data/train.csv',\n",
    "                        'test': 's3://sagemaker-us-east-2-920979387335/sentiment_data/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 268 ms, sys: 10.4 ms, total: 278 ms\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = sklearn_estimator.deploy(instance_type='ml.m4.xlarge', #'ml.p2.xlarge',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# First: generate predicted, class labels\n",
    "y_test_preds = predictor.predict(X_test)\n",
    "\n",
    "# test that your model generates the correct number of labels\n",
    "assert len(y_test_preds)==len(y_test), 'Unexpected number of predictions.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "0.735474006116208\n",
      "\n",
      "F1 macro score:\n",
      "0.5813747503888349\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.22      0.32       180\n",
      "           1       0.76      0.92      0.83       832\n",
      "           2       0.69      0.52      0.59       296\n",
      "\n",
      "    accuracy                           0.74      1308\n",
      "   macro avg       0.67      0.56      0.58      1308\n",
      "weighted avg       0.72      0.74      0.71      1308\n",
      "\n",
      "\n",
      "Predicted class labels: \n",
      "[1 1 1 ... 1 1 2]\n",
      "\n",
      "True class labels: \n",
      "[1 1 1 ... 0 1 2]\n",
      "\n",
      " Confusion matrix:\n",
      "[[0.22222222 0.65555556 0.12222222]\n",
      " [0.02163462 0.92307692 0.05528846]\n",
      " [0.0472973  0.43243243 0.52027027]]\n"
     ]
    }
   ],
   "source": [
    "# Second: calculate the test accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print('\\nAccuracy:')\n",
    "print(accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "print('\\nF1 macro score:')\n",
    "print(f1_score(y_test, y_test_preds, average='macro'))\n",
    "      \n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_test_preds))\n",
    "      \n",
    "## print out the array of predicted and true labels, if you want\n",
    "print('\\nPredicted class labels: ')\n",
    "print(y_test_preds)\n",
    "print('\\nTrue class labels: ')\n",
    "print(y_test.values)\n",
    "\n",
    "print('\\n Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_test_preds, normalize='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and fill in the line below!\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
